{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bbf140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d87afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.58MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 11.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.69MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f249178e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(mnist_dataset[i][0].squeeze().numpy(), cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {mnist_dataset[i][1]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ccb2d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 60000 || Image Shape: torch.Size([1, 28, 28])\n",
      "Labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "num_images = len(mnist_dataset)\n",
    "image_size = mnist_dataset[0][0].shape\n",
    "unique_labels = set([mnist_dataset[i][1] for i in range(num_images)])\n",
    "\n",
    "print(f'Dataset Size: {num_images} || Image Shape: {image_size}')\n",
    "print(f'Labels: {unique_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c605b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Dimensions: 48000 x torch.Size([1, 28, 28])\n",
      "Test Dataset Dimensions: 12000 x torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def test_train_split(train_ratio = 0.8, data = mnist_dataset):\n",
    "    train_size = int(train_ratio * len(mnist_dataset))\n",
    "    test_size = len(mnist_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(data, [train_size, test_size])\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "trainData, testData = test_train_split()\n",
    "print(f'Train Dataset Dimensions: {len(trainData)} x {trainData[0][0].shape}')\n",
    "print(f'Test Dataset Dimensions: {len(testData)} x {testData[0][0].shape}')\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testData, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "279fc670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our output is tensor([-0.0784, -0.3512, -0.1324,  0.0929, -0.1764], grad_fn=<ViewBackward0>)!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "network = LinearNetwork(10, 20, 5)\n",
    "\n",
    "input = torch.randn(10)\n",
    "\n",
    "output = network(input)\n",
    "\n",
    "print(f\"Our output is {output}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13f248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our output is tensor([-0.1571,  0.0123, -0.2828,  0.0445,  0.0428], grad_fn=<ViewBackward0>)!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NetworkWithReLU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x= self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "network = NetworkWithReLU(10, 20, 5)\n",
    "input = torch.randn(10)\n",
    "output = network(input)\n",
    "print(f\"Our output is {output}!\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "some_model = NetworkWithReLU(10, 20, 5)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(some_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "input_size = 28 * 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "model = MNISTNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b184e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train Loss: 1.389828 // Val Loss: 0.705435 // Train Acc: 71.74% // Val Acc: 84.42%\n",
      "Epoch  2: Train Loss: 0.561517 // Val Loss: 0.462248 // Train Acc: 86.39% // Val Acc: 88.07%\n",
      "Epoch  3: Train Loss: 0.429468 // Val Loss: 0.392267 // Train Acc: 88.61% // Val Acc: 89.22%\n",
      "Epoch  4: Train Loss: 0.379308 // Val Loss: 0.358489 // Train Acc: 89.54% // Val Acc: 90.03%\n",
      "Epoch  5: Train Loss: 0.351377 // Val Loss: 0.336988 // Train Acc: 90.21% // Val Acc: 90.49%\n",
      "Epoch  6: Train Loss: 0.332059 // Val Loss: 0.320809 // Train Acc: 90.64% // Val Acc: 91.04%\n",
      "Epoch  7: Train Loss: 0.317096 // Val Loss: 0.308065 // Train Acc: 91.08% // Val Acc: 91.21%\n",
      "Epoch  8: Train Loss: 0.304636 // Val Loss: 0.298257 // Train Acc: 91.36% // Val Acc: 91.68%\n",
      "Epoch  9: Train Loss: 0.293633 // Val Loss: 0.288982 // Train Acc: 91.67% // Val Acc: 91.99%\n",
      "Epoch 10: Train Loss: 0.284030 // Val Loss: 0.279893 // Train Acc: 91.97% // Val Acc: 92.22%\n",
      "Epoch 11: Train Loss: 0.274847 // Val Loss: 0.273112 // Train Acc: 92.29% // Val Acc: 92.43%\n",
      "Epoch 12: Train Loss: 0.266301 // Val Loss: 0.264694 // Train Acc: 92.51% // Val Acc: 92.59%\n",
      "Epoch 13: Train Loss: 0.258416 // Val Loss: 0.257919 // Train Acc: 92.69% // Val Acc: 92.82%\n",
      "Epoch 14: Train Loss: 0.250898 // Val Loss: 0.251485 // Train Acc: 92.94% // Val Acc: 93.08%\n",
      "Epoch 15: Train Loss: 0.243786 // Val Loss: 0.245507 // Train Acc: 93.12% // Val Acc: 93.32%\n",
      "Epoch 16: Train Loss: 0.237097 // Val Loss: 0.240784 // Train Acc: 93.28% // Val Acc: 93.22%\n",
      "Epoch 17: Train Loss: 0.230528 // Val Loss: 0.234548 // Train Acc: 93.47% // Val Acc: 93.55%\n",
      "Epoch 18: Train Loss: 0.224645 // Val Loss: 0.229686 // Train Acc: 93.65% // Val Acc: 93.74%\n",
      "Epoch 19: Train Loss: 0.218982 // Val Loss: 0.225246 // Train Acc: 93.83% // Val Acc: 93.86%\n",
      "Epoch 20: Train Loss: 0.213416 // Val Loss: 0.220032 // Train Acc: 94.04% // Val Acc: 93.97%\n",
      "Epoch 21: Train Loss: 0.208207 // Val Loss: 0.215536 // Train Acc: 94.16% // Val Acc: 94.09%\n",
      "Epoch 22: Train Loss: 0.203327 // Val Loss: 0.211433 // Train Acc: 94.34% // Val Acc: 94.23%\n",
      "Epoch 23: Train Loss: 0.198489 // Val Loss: 0.208011 // Train Acc: 94.49% // Val Acc: 94.37%\n",
      "Epoch 24: Train Loss: 0.194076 // Val Loss: 0.203452 // Train Acc: 94.56% // Val Acc: 94.42%\n",
      "Epoch 25: Train Loss: 0.189798 // Val Loss: 0.200762 // Train Acc: 94.69% // Val Acc: 94.58%\n",
      "Epoch 26: Train Loss: 0.185614 // Val Loss: 0.197098 // Train Acc: 94.80% // Val Acc: 94.47%\n",
      "Epoch 27: Train Loss: 0.181524 // Val Loss: 0.193060 // Train Acc: 94.92% // Val Acc: 94.72%\n",
      "Epoch 28: Train Loss: 0.177705 // Val Loss: 0.189997 // Train Acc: 95.04% // Val Acc: 94.78%\n",
      "Epoch 29: Train Loss: 0.173841 // Val Loss: 0.186234 // Train Acc: 95.13% // Val Acc: 94.77%\n",
      "Epoch 30: Train Loss: 0.170298 // Val Loss: 0.183914 // Train Acc: 95.25% // Val Acc: 94.91%\n",
      "Epoch 31: Train Loss: 0.166845 // Val Loss: 0.180262 // Train Acc: 95.31% // Val Acc: 94.97%\n",
      "Epoch 32: Train Loss: 0.163565 // Val Loss: 0.178018 // Train Acc: 95.40% // Val Acc: 94.97%\n",
      "Epoch 33: Train Loss: 0.160286 // Val Loss: 0.175440 // Train Acc: 95.49% // Val Acc: 95.03%\n",
      "Epoch 34: Train Loss: 0.157104 // Val Loss: 0.172892 // Train Acc: 95.63% // Val Acc: 95.17%\n",
      "Epoch 35: Train Loss: 0.154138 // Val Loss: 0.169780 // Train Acc: 95.68% // Val Acc: 95.17%\n",
      "Epoch 36: Train Loss: 0.151215 // Val Loss: 0.167991 // Train Acc: 95.73% // Val Acc: 95.28%\n",
      "Epoch 37: Train Loss: 0.148300 // Val Loss: 0.165588 // Train Acc: 95.84% // Val Acc: 95.38%\n",
      "Epoch 38: Train Loss: 0.145696 // Val Loss: 0.162702 // Train Acc: 95.96% // Val Acc: 95.47%\n",
      "Epoch 39: Train Loss: 0.143056 // Val Loss: 0.160631 // Train Acc: 95.98% // Val Acc: 95.53%\n",
      "Epoch 40: Train Loss: 0.140527 // Val Loss: 0.158150 // Train Acc: 96.05% // Val Acc: 95.58%\n",
      "Epoch 41: Train Loss: 0.138058 // Val Loss: 0.156390 // Train Acc: 96.09% // Val Acc: 95.72%\n",
      "Epoch 42: Train Loss: 0.135564 // Val Loss: 0.154153 // Train Acc: 96.21% // Val Acc: 95.67%\n",
      "Epoch 43: Train Loss: 0.133324 // Val Loss: 0.152157 // Train Acc: 96.25% // Val Acc: 95.78%\n",
      "Epoch 44: Train Loss: 0.131145 // Val Loss: 0.150195 // Train Acc: 96.31% // Val Acc: 95.78%\n",
      "Epoch 45: Train Loss: 0.128994 // Val Loss: 0.148798 // Train Acc: 96.38% // Val Acc: 95.86%\n",
      "Epoch 46: Train Loss: 0.126724 // Val Loss: 0.147075 // Train Acc: 96.45% // Val Acc: 95.93%\n",
      "Epoch 47: Train Loss: 0.124787 // Val Loss: 0.145000 // Train Acc: 96.50% // Val Acc: 95.98%\n",
      "Epoch 48: Train Loss: 0.122766 // Val Loss: 0.143762 // Train Acc: 96.59% // Val Acc: 96.07%\n",
      "Epoch 49: Train Loss: 0.120880 // Val Loss: 0.141756 // Train Acc: 96.62% // Val Acc: 96.05%\n",
      "Epoch 50: Train Loss: 0.118960 // Val Loss: 0.140794 // Train Acc: 96.66% // Val Acc: 96.12%\n",
      "Epoch 51: Train Loss: 0.117093 // Val Loss: 0.139149 // Train Acc: 96.73% // Val Acc: 96.17%\n",
      "Epoch 52: Train Loss: 0.115333 // Val Loss: 0.137711 // Train Acc: 96.77% // Val Acc: 96.15%\n",
      "Epoch 53: Train Loss: 0.113603 // Val Loss: 0.136427 // Train Acc: 96.84% // Val Acc: 96.28%\n",
      "Epoch 54: Train Loss: 0.111938 // Val Loss: 0.134576 // Train Acc: 96.91% // Val Acc: 96.32%\n",
      "Epoch 55: Train Loss: 0.110261 // Val Loss: 0.133617 // Train Acc: 96.92% // Val Acc: 96.32%\n",
      "Epoch 56: Train Loss: 0.108616 // Val Loss: 0.132080 // Train Acc: 97.03% // Val Acc: 96.33%\n",
      "Epoch 57: Train Loss: 0.107025 // Val Loss: 0.131571 // Train Acc: 97.02% // Val Acc: 96.40%\n",
      "Epoch 58: Train Loss: 0.105544 // Val Loss: 0.130099 // Train Acc: 97.08% // Val Acc: 96.42%\n",
      "Epoch 59: Train Loss: 0.104098 // Val Loss: 0.128591 // Train Acc: 97.14% // Val Acc: 96.50%\n",
      "Epoch 60: Train Loss: 0.102545 // Val Loss: 0.127647 // Train Acc: 97.19% // Val Acc: 96.46%\n",
      "Epoch 61: Train Loss: 0.101347 // Val Loss: 0.126589 // Train Acc: 97.23% // Val Acc: 96.44%\n",
      "Epoch 62: Train Loss: 0.099842 // Val Loss: 0.126007 // Train Acc: 97.23% // Val Acc: 96.50%\n",
      "Epoch 63: Train Loss: 0.098521 // Val Loss: 0.124686 // Train Acc: 97.29% // Val Acc: 96.53%\n",
      "Epoch 64: Train Loss: 0.097084 // Val Loss: 0.123370 // Train Acc: 97.34% // Val Acc: 96.57%\n",
      "Epoch 65: Train Loss: 0.095832 // Val Loss: 0.122993 // Train Acc: 97.36% // Val Acc: 96.63%\n",
      "Epoch 66: Train Loss: 0.094579 // Val Loss: 0.121601 // Train Acc: 97.41% // Val Acc: 96.54%\n",
      "Epoch 67: Train Loss: 0.093378 // Val Loss: 0.120508 // Train Acc: 97.50% // Val Acc: 96.72%\n",
      "Epoch 68: Train Loss: 0.092156 // Val Loss: 0.120837 // Train Acc: 97.45% // Val Acc: 96.58%\n",
      "Epoch 69: Train Loss: 0.090953 // Val Loss: 0.118965 // Train Acc: 97.52% // Val Acc: 96.65%\n",
      "Epoch 70: Train Loss: 0.089751 // Val Loss: 0.118334 // Train Acc: 97.55% // Val Acc: 96.63%\n",
      "Epoch 71: Train Loss: 0.088731 // Val Loss: 0.117159 // Train Acc: 97.58% // Val Acc: 96.70%\n",
      "Epoch 72: Train Loss: 0.087593 // Val Loss: 0.116416 // Train Acc: 97.60% // Val Acc: 96.71%\n",
      "Epoch 73: Train Loss: 0.086520 // Val Loss: 0.115449 // Train Acc: 97.62% // Val Acc: 96.78%\n",
      "Epoch 74: Train Loss: 0.085471 // Val Loss: 0.115448 // Train Acc: 97.65% // Val Acc: 96.82%\n",
      "Epoch 75: Train Loss: 0.084350 // Val Loss: 0.113813 // Train Acc: 97.66% // Val Acc: 96.77%\n",
      "Epoch 76: Train Loss: 0.083361 // Val Loss: 0.113982 // Train Acc: 97.75% // Val Acc: 96.72%\n",
      "Epoch 77: Train Loss: 0.082307 // Val Loss: 0.113264 // Train Acc: 97.76% // Val Acc: 96.74%\n",
      "Epoch 78: Train Loss: 0.081310 // Val Loss: 0.112035 // Train Acc: 97.79% // Val Acc: 96.79%\n",
      "Epoch 79: Train Loss: 0.080447 // Val Loss: 0.111778 // Train Acc: 97.81% // Val Acc: 96.80%\n",
      "Epoch 80: Train Loss: 0.079534 // Val Loss: 0.110803 // Train Acc: 97.85% // Val Acc: 96.88%\n",
      "Epoch 81: Train Loss: 0.078478 // Val Loss: 0.110150 // Train Acc: 97.85% // Val Acc: 96.89%\n",
      "Epoch 82: Train Loss: 0.077580 // Val Loss: 0.109449 // Train Acc: 97.91% // Val Acc: 96.89%\n",
      "Epoch 83: Train Loss: 0.076704 // Val Loss: 0.108828 // Train Acc: 97.90% // Val Acc: 96.88%\n",
      "Epoch 84: Train Loss: 0.075796 // Val Loss: 0.108153 // Train Acc: 97.95% // Val Acc: 96.92%\n",
      "Epoch 85: Train Loss: 0.075033 // Val Loss: 0.107406 // Train Acc: 97.99% // Val Acc: 96.98%\n",
      "Epoch 86: Train Loss: 0.074137 // Val Loss: 0.107296 // Train Acc: 97.99% // Val Acc: 96.96%\n",
      "Epoch 87: Train Loss: 0.073335 // Val Loss: 0.106493 // Train Acc: 98.02% // Val Acc: 96.96%\n",
      "Epoch 88: Train Loss: 0.072370 // Val Loss: 0.106614 // Train Acc: 98.03% // Val Acc: 96.93%\n",
      "Epoch 89: Train Loss: 0.071689 // Val Loss: 0.105659 // Train Acc: 98.07% // Val Acc: 97.02%\n",
      "Epoch 90: Train Loss: 0.070918 // Val Loss: 0.105600 // Train Acc: 98.10% // Val Acc: 97.03%\n",
      "Epoch 91: Train Loss: 0.070128 // Val Loss: 0.104894 // Train Acc: 98.13% // Val Acc: 97.04%\n",
      "Epoch 92: Train Loss: 0.069254 // Val Loss: 0.103778 // Train Acc: 98.19% // Val Acc: 97.08%\n",
      "Epoch 93: Train Loss: 0.068625 // Val Loss: 0.103536 // Train Acc: 98.20% // Val Acc: 97.04%\n",
      "Epoch 94: Train Loss: 0.067855 // Val Loss: 0.103064 // Train Acc: 98.23% // Val Acc: 97.03%\n",
      "Epoch 95: Train Loss: 0.067130 // Val Loss: 0.102888 // Train Acc: 98.23% // Val Acc: 97.11%\n",
      "Epoch 96: Train Loss: 0.066400 // Val Loss: 0.102680 // Train Acc: 98.23% // Val Acc: 97.10%\n",
      "Epoch 97: Train Loss: 0.065656 // Val Loss: 0.102120 // Train Acc: 98.29% // Val Acc: 97.01%\n",
      "Epoch 98: Train Loss: 0.064897 // Val Loss: 0.101366 // Train Acc: 98.30% // Val Acc: 97.09%\n",
      "Epoch 99: Train Loss: 0.064296 // Val Loss: 0.101237 // Train Acc: 98.34% // Val Acc: 97.10%\n",
      "Epoch 100: Train Loss: 0.063666 // Val Loss: 0.100868 // Train Acc: 98.35% // Val Acc: 97.17%\n",
      "Epoch 101: Train Loss: 0.063003 // Val Loss: 0.100365 // Train Acc: 98.36% // Val Acc: 97.11%\n",
      "Epoch 102: Train Loss: 0.062400 // Val Loss: 0.100381 // Train Acc: 98.41% // Val Acc: 97.07%\n",
      "Epoch 103: Train Loss: 0.061749 // Val Loss: 0.100279 // Train Acc: 98.43% // Val Acc: 97.03%\n",
      "Epoch 104: Train Loss: 0.061185 // Val Loss: 0.099181 // Train Acc: 98.41% // Val Acc: 97.11%\n",
      "Epoch 105: Train Loss: 0.060493 // Val Loss: 0.099038 // Train Acc: 98.45% // Val Acc: 97.13%\n",
      "Epoch 106: Train Loss: 0.059846 // Val Loss: 0.098782 // Train Acc: 98.47% // Val Acc: 97.22%\n",
      "Epoch 107: Train Loss: 0.059323 // Val Loss: 0.098309 // Train Acc: 98.50% // Val Acc: 97.15%\n",
      "Epoch 108: Train Loss: 0.058694 // Val Loss: 0.098095 // Train Acc: 98.53% // Val Acc: 97.09%\n",
      "Epoch 109: Train Loss: 0.058147 // Val Loss: 0.097655 // Train Acc: 98.54% // Val Acc: 97.12%\n",
      "Epoch 110: Train Loss: 0.057523 // Val Loss: 0.097162 // Train Acc: 98.54% // Val Acc: 97.13%\n",
      "Epoch 111: Train Loss: 0.056918 // Val Loss: 0.097038 // Train Acc: 98.56% // Val Acc: 97.12%\n",
      "Epoch 112: Train Loss: 0.056492 // Val Loss: 0.096714 // Train Acc: 98.56% // Val Acc: 97.17%\n",
      "Epoch 113: Train Loss: 0.055966 // Val Loss: 0.096286 // Train Acc: 98.61% // Val Acc: 97.22%\n",
      "Epoch 114: Train Loss: 0.055369 // Val Loss: 0.095848 // Train Acc: 98.62% // Val Acc: 97.17%\n",
      "Epoch 115: Train Loss: 0.054760 // Val Loss: 0.096260 // Train Acc: 98.63% // Val Acc: 97.11%\n",
      "Epoch 116: Train Loss: 0.054394 // Val Loss: 0.095521 // Train Acc: 98.64% // Val Acc: 97.24%\n",
      "Epoch 117: Train Loss: 0.053763 // Val Loss: 0.095373 // Train Acc: 98.66% // Val Acc: 97.16%\n",
      "Epoch 118: Train Loss: 0.053264 // Val Loss: 0.094485 // Train Acc: 98.64% // Val Acc: 97.26%\n",
      "Epoch 119: Train Loss: 0.052787 // Val Loss: 0.094271 // Train Acc: 98.68% // Val Acc: 97.28%\n",
      "Epoch 120: Train Loss: 0.052271 // Val Loss: 0.094510 // Train Acc: 98.72% // Val Acc: 97.17%\n",
      "Epoch 121: Train Loss: 0.051850 // Val Loss: 0.094377 // Train Acc: 98.74% // Val Acc: 97.21%\n",
      "Epoch 122: Train Loss: 0.051352 // Val Loss: 0.094399 // Train Acc: 98.73% // Val Acc: 97.24%\n",
      "Epoch 123: Train Loss: 0.050856 // Val Loss: 0.093845 // Train Acc: 98.78% // Val Acc: 97.17%\n",
      "Epoch 124: Train Loss: 0.050356 // Val Loss: 0.093585 // Train Acc: 98.79% // Val Acc: 97.22%\n",
      "Epoch 125: Train Loss: 0.049854 // Val Loss: 0.093530 // Train Acc: 98.76% // Val Acc: 97.22%\n",
      "Epoch 126: Train Loss: 0.049561 // Val Loss: 0.092922 // Train Acc: 98.80% // Val Acc: 97.28%\n",
      "Epoch 127: Train Loss: 0.049045 // Val Loss: 0.093024 // Train Acc: 98.81% // Val Acc: 97.22%\n",
      "Epoch 128: Train Loss: 0.048620 // Val Loss: 0.092681 // Train Acc: 98.83% // Val Acc: 97.26%\n",
      "Epoch 129: Train Loss: 0.048149 // Val Loss: 0.092631 // Train Acc: 98.85% // Val Acc: 97.28%\n",
      "Epoch 130: Train Loss: 0.047717 // Val Loss: 0.092330 // Train Acc: 98.87% // Val Acc: 97.28%\n",
      "Epoch 131: Train Loss: 0.047318 // Val Loss: 0.092171 // Train Acc: 98.88% // Val Acc: 97.19%\n",
      "Epoch 132: Train Loss: 0.046865 // Val Loss: 0.091654 // Train Acc: 98.89% // Val Acc: 97.20%\n",
      "Epoch 133: Train Loss: 0.046509 // Val Loss: 0.091878 // Train Acc: 98.92% // Val Acc: 97.25%\n",
      "Epoch 134: Train Loss: 0.046085 // Val Loss: 0.091491 // Train Acc: 98.92% // Val Acc: 97.33%\n",
      "Epoch 135: Train Loss: 0.045710 // Val Loss: 0.091414 // Train Acc: 98.93% // Val Acc: 97.30%\n",
      "Epoch 136: Train Loss: 0.045287 // Val Loss: 0.091364 // Train Acc: 98.93% // Val Acc: 97.23%\n",
      "Epoch 137: Train Loss: 0.044806 // Val Loss: 0.090701 // Train Acc: 99.00% // Val Acc: 97.33%\n",
      "Epoch 138: Train Loss: 0.044486 // Val Loss: 0.090498 // Train Acc: 98.98% // Val Acc: 97.26%\n",
      "Epoch 139: Train Loss: 0.044093 // Val Loss: 0.090912 // Train Acc: 99.01% // Val Acc: 97.33%\n",
      "Epoch 140: Train Loss: 0.043721 // Val Loss: 0.090511 // Train Acc: 99.02% // Val Acc: 97.36%\n",
      "Epoch 141: Train Loss: 0.043341 // Val Loss: 0.090218 // Train Acc: 99.03% // Val Acc: 97.30%\n",
      "Epoch 142: Train Loss: 0.042971 // Val Loss: 0.090824 // Train Acc: 99.01% // Val Acc: 97.28%\n",
      "Epoch 143: Train Loss: 0.042659 // Val Loss: 0.089599 // Train Acc: 99.04% // Val Acc: 97.33%\n",
      "Epoch 144: Train Loss: 0.042312 // Val Loss: 0.089436 // Train Acc: 99.05% // Val Acc: 97.38%\n",
      "Epoch 145: Train Loss: 0.041874 // Val Loss: 0.089290 // Train Acc: 99.09% // Val Acc: 97.33%\n",
      "Epoch 146: Train Loss: 0.041524 // Val Loss: 0.089414 // Train Acc: 99.09% // Val Acc: 97.34%\n",
      "Epoch 147: Train Loss: 0.041195 // Val Loss: 0.089046 // Train Acc: 99.07% // Val Acc: 97.36%\n",
      "Epoch 148: Train Loss: 0.040809 // Val Loss: 0.089740 // Train Acc: 99.08% // Val Acc: 97.35%\n",
      "Epoch 149: Train Loss: 0.040529 // Val Loss: 0.088843 // Train Acc: 99.12% // Val Acc: 97.37%\n",
      "Epoch 150: Train Loss: 0.040168 // Val Loss: 0.088799 // Train Acc: 99.14% // Val Acc: 97.40%\n",
      "Epoch 151: Train Loss: 0.039861 // Val Loss: 0.088744 // Train Acc: 99.16% // Val Acc: 97.36%\n",
      "Epoch 152: Train Loss: 0.039472 // Val Loss: 0.088326 // Train Acc: 99.14% // Val Acc: 97.38%\n",
      "Epoch 153: Train Loss: 0.039192 // Val Loss: 0.088404 // Train Acc: 99.19% // Val Acc: 97.37%\n",
      "Epoch 154: Train Loss: 0.038832 // Val Loss: 0.088368 // Train Acc: 99.17% // Val Acc: 97.38%\n",
      "Epoch 155: Train Loss: 0.038521 // Val Loss: 0.088064 // Train Acc: 99.20% // Val Acc: 97.38%\n",
      "Epoch 156: Train Loss: 0.038147 // Val Loss: 0.087930 // Train Acc: 99.22% // Val Acc: 97.38%\n",
      "Epoch 157: Train Loss: 0.037865 // Val Loss: 0.087870 // Train Acc: 99.18% // Val Acc: 97.42%\n",
      "Epoch 158: Train Loss: 0.037550 // Val Loss: 0.087607 // Train Acc: 99.22% // Val Acc: 97.36%\n",
      "Epoch 159: Train Loss: 0.037297 // Val Loss: 0.087868 // Train Acc: 99.23% // Val Acc: 97.38%\n",
      "Epoch 160: Train Loss: 0.036983 // Val Loss: 0.087972 // Train Acc: 99.24% // Val Acc: 97.33%\n",
      "Epoch 161: Train Loss: 0.036693 // Val Loss: 0.087565 // Train Acc: 99.25% // Val Acc: 97.35%\n",
      "Epoch 162: Train Loss: 0.036335 // Val Loss: 0.087601 // Train Acc: 99.26% // Val Acc: 97.33%\n",
      "Epoch 163: Train Loss: 0.036137 // Val Loss: 0.087274 // Train Acc: 99.28% // Val Acc: 97.39%\n",
      "Epoch 164: Train Loss: 0.035791 // Val Loss: 0.087435 // Train Acc: 99.28% // Val Acc: 97.42%\n",
      "Epoch 165: Train Loss: 0.035524 // Val Loss: 0.086912 // Train Acc: 99.28% // Val Acc: 97.41%\n",
      "Epoch 166: Train Loss: 0.035286 // Val Loss: 0.087112 // Train Acc: 99.28% // Val Acc: 97.42%\n",
      "Epoch 167: Train Loss: 0.034956 // Val Loss: 0.086692 // Train Acc: 99.30% // Val Acc: 97.42%\n",
      "Epoch 168: Train Loss: 0.034707 // Val Loss: 0.086580 // Train Acc: 99.32% // Val Acc: 97.44%\n",
      "Epoch 169: Train Loss: 0.034445 // Val Loss: 0.086503 // Train Acc: 99.34% // Val Acc: 97.44%\n",
      "Epoch 170: Train Loss: 0.034087 // Val Loss: 0.086894 // Train Acc: 99.31% // Val Acc: 97.38%\n",
      "Epoch 171: Train Loss: 0.033892 // Val Loss: 0.086356 // Train Acc: 99.34% // Val Acc: 97.45%\n",
      "Epoch 172: Train Loss: 0.033614 // Val Loss: 0.086642 // Train Acc: 99.34% // Val Acc: 97.37%\n",
      "Epoch 173: Train Loss: 0.033393 // Val Loss: 0.086167 // Train Acc: 99.36% // Val Acc: 97.43%\n",
      "Epoch 174: Train Loss: 0.033072 // Val Loss: 0.086146 // Train Acc: 99.37% // Val Acc: 97.40%\n",
      "Epoch 175: Train Loss: 0.032821 // Val Loss: 0.086042 // Train Acc: 99.38% // Val Acc: 97.43%\n",
      "Epoch 176: Train Loss: 0.032598 // Val Loss: 0.086183 // Train Acc: 99.41% // Val Acc: 97.43%\n",
      "Epoch 177: Train Loss: 0.032335 // Val Loss: 0.086137 // Train Acc: 99.39% // Val Acc: 97.39%\n",
      "Epoch 178: Train Loss: 0.032078 // Val Loss: 0.086139 // Train Acc: 99.40% // Val Acc: 97.45%\n",
      "Epoch 179: Train Loss: 0.031807 // Val Loss: 0.085906 // Train Acc: 99.40% // Val Acc: 97.50%\n",
      "Epoch 180: Train Loss: 0.031524 // Val Loss: 0.086487 // Train Acc: 99.41% // Val Acc: 97.37%\n",
      "Epoch 181: Train Loss: 0.031349 // Val Loss: 0.086108 // Train Acc: 99.41% // Val Acc: 97.44%\n",
      "Epoch 182: Train Loss: 0.031103 // Val Loss: 0.086253 // Train Acc: 99.42% // Val Acc: 97.42%\n",
      "Epoch 183: Train Loss: 0.030828 // Val Loss: 0.085541 // Train Acc: 99.44% // Val Acc: 97.44%\n",
      "Epoch 184: Train Loss: 0.030683 // Val Loss: 0.085439 // Train Acc: 99.44% // Val Acc: 97.42%\n",
      "Epoch 185: Train Loss: 0.030405 // Val Loss: 0.085314 // Train Acc: 99.43% // Val Acc: 97.43%\n",
      "Epoch 186: Train Loss: 0.030196 // Val Loss: 0.085826 // Train Acc: 99.45% // Val Acc: 97.43%\n",
      "Epoch 187: Train Loss: 0.029945 // Val Loss: 0.085932 // Train Acc: 99.47% // Val Acc: 97.42%\n",
      "Epoch 188: Train Loss: 0.029749 // Val Loss: 0.085514 // Train Acc: 99.47% // Val Acc: 97.43%\n",
      "Epoch 189: Train Loss: 0.029456 // Val Loss: 0.085483 // Train Acc: 99.49% // Val Acc: 97.40%\n",
      "Epoch 190: Train Loss: 0.029297 // Val Loss: 0.085422 // Train Acc: 99.46% // Val Acc: 97.42%\n",
      "Epoch 191: Train Loss: 0.029045 // Val Loss: 0.085188 // Train Acc: 99.47% // Val Acc: 97.42%\n",
      "Epoch 192: Train Loss: 0.028820 // Val Loss: 0.084851 // Train Acc: 99.51% // Val Acc: 97.47%\n",
      "Epoch 193: Train Loss: 0.028645 // Val Loss: 0.084954 // Train Acc: 99.49% // Val Acc: 97.47%\n",
      "Epoch 194: Train Loss: 0.028421 // Val Loss: 0.084975 // Train Acc: 99.51% // Val Acc: 97.39%\n",
      "Epoch 195: Train Loss: 0.028241 // Val Loss: 0.085325 // Train Acc: 99.52% // Val Acc: 97.42%\n",
      "Epoch 196: Train Loss: 0.028002 // Val Loss: 0.085007 // Train Acc: 99.53% // Val Acc: 97.42%\n",
      "Epoch 197: Train Loss: 0.027803 // Val Loss: 0.084825 // Train Acc: 99.52% // Val Acc: 97.42%\n",
      "Epoch 198: Train Loss: 0.027611 // Val Loss: 0.084753 // Train Acc: 99.52% // Val Acc: 97.47%\n",
      "Epoch 199: Train Loss: 0.027360 // Val Loss: 0.084673 // Train Acc: 99.53% // Val Acc: 97.47%\n",
      "Epoch 200: Train Loss: 0.027169 // Val Loss: 0.084901 // Train Acc: 99.55% // Val Acc: 97.42%\n",
      "Epoch 201: Train Loss: 0.026979 // Val Loss: 0.084691 // Train Acc: 99.55% // Val Acc: 97.44%\n",
      "Epoch 202: Train Loss: 0.026771 // Val Loss: 0.084658 // Train Acc: 99.57% // Val Acc: 97.43%\n",
      "Epoch 203: Train Loss: 0.026605 // Val Loss: 0.084700 // Train Acc: 99.55% // Val Acc: 97.49%\n",
      "Epoch 204: Train Loss: 0.026409 // Val Loss: 0.084636 // Train Acc: 99.55% // Val Acc: 97.44%\n",
      "Epoch 205: Train Loss: 0.026222 // Val Loss: 0.084508 // Train Acc: 99.57% // Val Acc: 97.51%\n",
      "Epoch 206: Train Loss: 0.025996 // Val Loss: 0.084709 // Train Acc: 99.59% // Val Acc: 97.45%\n",
      "Epoch 207: Train Loss: 0.025825 // Val Loss: 0.084787 // Train Acc: 99.58% // Val Acc: 97.42%\n",
      "Epoch 208: Train Loss: 0.025605 // Val Loss: 0.084780 // Train Acc: 99.58% // Val Acc: 97.47%\n",
      "Epoch 209: Train Loss: 0.025482 // Val Loss: 0.084422 // Train Acc: 99.59% // Val Acc: 97.49%\n",
      "Epoch 210: Train Loss: 0.025255 // Val Loss: 0.084840 // Train Acc: 99.59% // Val Acc: 97.50%\n",
      "Epoch 211: Train Loss: 0.025145 // Val Loss: 0.084411 // Train Acc: 99.59% // Val Acc: 97.44%\n",
      "Epoch 212: Train Loss: 0.024871 // Val Loss: 0.084384 // Train Acc: 99.61% // Val Acc: 97.42%\n",
      "Epoch 213: Train Loss: 0.024744 // Val Loss: 0.084561 // Train Acc: 99.61% // Val Acc: 97.44%\n",
      "Epoch 214: Train Loss: 0.024522 // Val Loss: 0.084102 // Train Acc: 99.62% // Val Acc: 97.50%\n",
      "Epoch 215: Train Loss: 0.024347 // Val Loss: 0.084449 // Train Acc: 99.62% // Val Acc: 97.50%\n",
      "Epoch 216: Train Loss: 0.024249 // Val Loss: 0.084187 // Train Acc: 99.62% // Val Acc: 97.45%\n",
      "Epoch 217: Train Loss: 0.024070 // Val Loss: 0.084248 // Train Acc: 99.61% // Val Acc: 97.53%\n",
      "Epoch 218: Train Loss: 0.023919 // Val Loss: 0.084325 // Train Acc: 99.62% // Val Acc: 97.47%\n",
      "Epoch 219: Train Loss: 0.023683 // Val Loss: 0.084773 // Train Acc: 99.65% // Val Acc: 97.46%\n",
      "Epoch 220: Train Loss: 0.023536 // Val Loss: 0.084171 // Train Acc: 99.64% // Val Acc: 97.45%\n",
      "Epoch 221: Train Loss: 0.023408 // Val Loss: 0.084435 // Train Acc: 99.64% // Val Acc: 97.42%\n",
      "Epoch 222: Train Loss: 0.023199 // Val Loss: 0.083876 // Train Acc: 99.65% // Val Acc: 97.50%\n",
      "Epoch 223: Train Loss: 0.023082 // Val Loss: 0.084228 // Train Acc: 99.66% // Val Acc: 97.47%\n",
      "Epoch 224: Train Loss: 0.022910 // Val Loss: 0.084352 // Train Acc: 99.65% // Val Acc: 97.48%\n",
      "Epoch 225: Train Loss: 0.022711 // Val Loss: 0.084081 // Train Acc: 99.65% // Val Acc: 97.51%\n",
      "Epoch 226: Train Loss: 0.022569 // Val Loss: 0.084519 // Train Acc: 99.67% // Val Acc: 97.44%\n",
      "Epoch 227: Train Loss: 0.022449 // Val Loss: 0.084219 // Train Acc: 99.66% // Val Acc: 97.47%\n",
      "Epoch 228: Train Loss: 0.022245 // Val Loss: 0.084332 // Train Acc: 99.66% // Val Acc: 97.47%\n",
      "Epoch 229: Train Loss: 0.022138 // Val Loss: 0.084195 // Train Acc: 99.67% // Val Acc: 97.50%\n",
      "Epoch 230: Train Loss: 0.021966 // Val Loss: 0.083869 // Train Acc: 99.67% // Val Acc: 97.52%\n",
      "Epoch 231: Train Loss: 0.021801 // Val Loss: 0.083953 // Train Acc: 99.67% // Val Acc: 97.47%\n",
      "Epoch 232: Train Loss: 0.021668 // Val Loss: 0.083891 // Train Acc: 99.70% // Val Acc: 97.50%\n",
      "Epoch 233: Train Loss: 0.021535 // Val Loss: 0.084115 // Train Acc: 99.68% // Val Acc: 97.50%\n",
      "Epoch 234: Train Loss: 0.021381 // Val Loss: 0.084426 // Train Acc: 99.69% // Val Acc: 97.44%\n",
      "Epoch 235: Train Loss: 0.021219 // Val Loss: 0.083866 // Train Acc: 99.69% // Val Acc: 97.53%\n",
      "Epoch 236: Train Loss: 0.021110 // Val Loss: 0.083859 // Train Acc: 99.70% // Val Acc: 97.48%\n",
      "Epoch 237: Train Loss: 0.020947 // Val Loss: 0.083902 // Train Acc: 99.70% // Val Acc: 97.50%\n",
      "Epoch 238: Train Loss: 0.020770 // Val Loss: 0.084086 // Train Acc: 99.72% // Val Acc: 97.49%\n",
      "Epoch 239: Train Loss: 0.020679 // Val Loss: 0.084177 // Train Acc: 99.69% // Val Acc: 97.47%\n",
      "Epoch 240: Train Loss: 0.020552 // Val Loss: 0.084344 // Train Acc: 99.70% // Val Acc: 97.47%\n",
      "Epoch 241: Train Loss: 0.020399 // Val Loss: 0.083774 // Train Acc: 99.71% // Val Acc: 97.54%\n",
      "Epoch 242: Train Loss: 0.020278 // Val Loss: 0.083874 // Train Acc: 99.71% // Val Acc: 97.52%\n",
      "Epoch 243: Train Loss: 0.020135 // Val Loss: 0.083878 // Train Acc: 99.72% // Val Acc: 97.52%\n",
      "Epoch 244: Train Loss: 0.019973 // Val Loss: 0.084414 // Train Acc: 99.72% // Val Acc: 97.43%\n",
      "Epoch 245: Train Loss: 0.019851 // Val Loss: 0.084537 // Train Acc: 99.72% // Val Acc: 97.47%\n",
      "Epoch 246: Train Loss: 0.019739 // Val Loss: 0.084117 // Train Acc: 99.73% // Val Acc: 97.50%\n",
      "Epoch 247: Train Loss: 0.019613 // Val Loss: 0.084089 // Train Acc: 99.72% // Val Acc: 97.48%\n",
      "Epoch 248: Train Loss: 0.019459 // Val Loss: 0.083742 // Train Acc: 99.73% // Val Acc: 97.53%\n",
      "Epoch 249: Train Loss: 0.019302 // Val Loss: 0.084055 // Train Acc: 99.74% // Val Acc: 97.58%\n",
      "Epoch 250: Train Loss: 0.019219 // Val Loss: 0.084177 // Train Acc: 99.75% // Val Acc: 97.46%\n",
      "Epoch 251: Train Loss: 0.019083 // Val Loss: 0.084144 // Train Acc: 99.75% // Val Acc: 97.48%\n",
      "Epoch 252: Train Loss: 0.018944 // Val Loss: 0.084046 // Train Acc: 99.75% // Val Acc: 97.52%\n",
      "Epoch 253: Train Loss: 0.018838 // Val Loss: 0.084489 // Train Acc: 99.76% // Val Acc: 97.47%\n",
      "Epoch 254: Train Loss: 0.018729 // Val Loss: 0.083815 // Train Acc: 99.75% // Val Acc: 97.54%\n",
      "Epoch 255: Train Loss: 0.018601 // Val Loss: 0.084011 // Train Acc: 99.75% // Val Acc: 97.53%\n",
      "Epoch 256: Train Loss: 0.018488 // Val Loss: 0.083820 // Train Acc: 99.78% // Val Acc: 97.49%\n",
      "Epoch 257: Train Loss: 0.018350 // Val Loss: 0.083991 // Train Acc: 99.76% // Val Acc: 97.44%\n",
      "Epoch 258: Train Loss: 0.018243 // Val Loss: 0.084122 // Train Acc: 99.78% // Val Acc: 97.53%\n",
      "Epoch 259: Train Loss: 0.018130 // Val Loss: 0.083864 // Train Acc: 99.78% // Val Acc: 97.47%\n",
      "Epoch 260: Train Loss: 0.017992 // Val Loss: 0.083815 // Train Acc: 99.77% // Val Acc: 97.52%\n",
      "Epoch 261: Train Loss: 0.017904 // Val Loss: 0.084272 // Train Acc: 99.78% // Val Acc: 97.47%\n",
      "Epoch 262: Train Loss: 0.017782 // Val Loss: 0.084583 // Train Acc: 99.79% // Val Acc: 97.47%\n",
      "Epoch 263: Train Loss: 0.017620 // Val Loss: 0.084602 // Train Acc: 99.79% // Val Acc: 97.58%\n",
      "Epoch 264: Train Loss: 0.017571 // Val Loss: 0.084092 // Train Acc: 99.79% // Val Acc: 97.51%\n",
      "Epoch 265: Train Loss: 0.017435 // Val Loss: 0.084089 // Train Acc: 99.81% // Val Acc: 97.54%\n",
      "Epoch 266: Train Loss: 0.017336 // Val Loss: 0.083918 // Train Acc: 99.80% // Val Acc: 97.53%\n",
      "Epoch 267: Train Loss: 0.017226 // Val Loss: 0.083980 // Train Acc: 99.80% // Val Acc: 97.56%\n",
      "Epoch 268: Train Loss: 0.017108 // Val Loss: 0.083984 // Train Acc: 99.81% // Val Acc: 97.54%\n",
      "Epoch 269: Train Loss: 0.017013 // Val Loss: 0.084120 // Train Acc: 99.81% // Val Acc: 97.55%\n",
      "Epoch 270: Train Loss: 0.016936 // Val Loss: 0.084177 // Train Acc: 99.81% // Val Acc: 97.50%\n",
      "Epoch 271: Train Loss: 0.016812 // Val Loss: 0.084084 // Train Acc: 99.82% // Val Acc: 97.58%\n",
      "Epoch 272: Train Loss: 0.016698 // Val Loss: 0.084556 // Train Acc: 99.83% // Val Acc: 97.53%\n",
      "Epoch 273: Train Loss: 0.016600 // Val Loss: 0.084320 // Train Acc: 99.82% // Val Acc: 97.52%\n",
      "Epoch 274: Train Loss: 0.016533 // Val Loss: 0.084192 // Train Acc: 99.82% // Val Acc: 97.51%\n",
      "Epoch 275: Train Loss: 0.016398 // Val Loss: 0.084213 // Train Acc: 99.84% // Val Acc: 97.56%\n",
      "Epoch 276: Train Loss: 0.016284 // Val Loss: 0.084418 // Train Acc: 99.83% // Val Acc: 97.52%\n",
      "Epoch 277: Train Loss: 0.016222 // Val Loss: 0.084348 // Train Acc: 99.83% // Val Acc: 97.49%\n",
      "Epoch 278: Train Loss: 0.016083 // Val Loss: 0.084118 // Train Acc: 99.83% // Val Acc: 97.57%\n",
      "Epoch 279: Train Loss: 0.015976 // Val Loss: 0.084180 // Train Acc: 99.84% // Val Acc: 97.52%\n",
      "Epoch 280: Train Loss: 0.015894 // Val Loss: 0.084316 // Train Acc: 99.83% // Val Acc: 97.51%\n",
      "Epoch 281: Train Loss: 0.015825 // Val Loss: 0.084389 // Train Acc: 99.84% // Val Acc: 97.52%\n",
      "Epoch 282: Train Loss: 0.015723 // Val Loss: 0.084014 // Train Acc: 99.85% // Val Acc: 97.57%\n",
      "Epoch 283: Train Loss: 0.015619 // Val Loss: 0.084331 // Train Acc: 99.84% // Val Acc: 97.53%\n",
      "Epoch 284: Train Loss: 0.015527 // Val Loss: 0.084306 // Train Acc: 99.85% // Val Acc: 97.56%\n",
      "Epoch 285: Train Loss: 0.015426 // Val Loss: 0.084443 // Train Acc: 99.85% // Val Acc: 97.56%\n",
      "Epoch 286: Train Loss: 0.015311 // Val Loss: 0.084860 // Train Acc: 99.85% // Val Acc: 97.51%\n",
      "Epoch 287: Train Loss: 0.015229 // Val Loss: 0.084325 // Train Acc: 99.85% // Val Acc: 97.57%\n",
      "Epoch 288: Train Loss: 0.015153 // Val Loss: 0.084363 // Train Acc: 99.85% // Val Acc: 97.53%\n",
      "Epoch 289: Train Loss: 0.015081 // Val Loss: 0.084397 // Train Acc: 99.85% // Val Acc: 97.52%\n",
      "Epoch 290: Train Loss: 0.014944 // Val Loss: 0.084787 // Train Acc: 99.86% // Val Acc: 97.55%\n",
      "Epoch 291: Train Loss: 0.014875 // Val Loss: 0.084448 // Train Acc: 99.86% // Val Acc: 97.51%\n",
      "Epoch 292: Train Loss: 0.014800 // Val Loss: 0.084600 // Train Acc: 99.85% // Val Acc: 97.54%\n",
      "Epoch 293: Train Loss: 0.014676 // Val Loss: 0.084176 // Train Acc: 99.85% // Val Acc: 97.53%\n",
      "Epoch 294: Train Loss: 0.014603 // Val Loss: 0.085193 // Train Acc: 99.86% // Val Acc: 97.50%\n",
      "Epoch 295: Train Loss: 0.014518 // Val Loss: 0.084470 // Train Acc: 99.86% // Val Acc: 97.54%\n",
      "Epoch 296: Train Loss: 0.014453 // Val Loss: 0.084727 // Train Acc: 99.88% // Val Acc: 97.53%\n",
      "Epoch 297: Train Loss: 0.014380 // Val Loss: 0.084762 // Train Acc: 99.86% // Val Acc: 97.53%\n",
      "Epoch 298: Train Loss: 0.014269 // Val Loss: 0.084677 // Train Acc: 99.88% // Val Acc: 97.50%\n",
      "Epoch 299: Train Loss: 0.014182 // Val Loss: 0.084489 // Train Acc: 99.88% // Val Acc: 97.55%\n",
      "Epoch 300: Train Loss: 0.014096 // Val Loss: 0.084748 // Train Acc: 99.87% // Val Acc: 97.58%\n",
      "Epoch 301: Train Loss: 0.014047 // Val Loss: 0.084666 // Train Acc: 99.88% // Val Acc: 97.55%\n",
      "Epoch 302: Train Loss: 0.013967 // Val Loss: 0.084804 // Train Acc: 99.88% // Val Acc: 97.54%\n",
      "Epoch 303: Train Loss: 0.013874 // Val Loss: 0.084739 // Train Acc: 99.88% // Val Acc: 97.56%\n",
      "Epoch 304: Train Loss: 0.013775 // Val Loss: 0.084796 // Train Acc: 99.89% // Val Acc: 97.55%\n",
      "Epoch 305: Train Loss: 0.013726 // Val Loss: 0.084923 // Train Acc: 99.89% // Val Acc: 97.54%\n",
      "Epoch 306: Train Loss: 0.013620 // Val Loss: 0.084748 // Train Acc: 99.89% // Val Acc: 97.57%\n",
      "Epoch 307: Train Loss: 0.013544 // Val Loss: 0.084976 // Train Acc: 99.88% // Val Acc: 97.52%\n",
      "Epoch 308: Train Loss: 0.013487 // Val Loss: 0.084736 // Train Acc: 99.88% // Val Acc: 97.55%\n",
      "Epoch 309: Train Loss: 0.013415 // Val Loss: 0.084967 // Train Acc: 99.89% // Val Acc: 97.51%\n",
      "Epoch 310: Train Loss: 0.013338 // Val Loss: 0.084886 // Train Acc: 99.89% // Val Acc: 97.56%\n",
      "Epoch 311: Train Loss: 0.013229 // Val Loss: 0.084784 // Train Acc: 99.89% // Val Acc: 97.57%\n",
      "Epoch 312: Train Loss: 0.013196 // Val Loss: 0.085309 // Train Acc: 99.90% // Val Acc: 97.53%\n",
      "Epoch 313: Train Loss: 0.013101 // Val Loss: 0.085337 // Train Acc: 99.90% // Val Acc: 97.56%\n",
      "Epoch 314: Train Loss: 0.013019 // Val Loss: 0.085052 // Train Acc: 99.89% // Val Acc: 97.56%\n",
      "Epoch 315: Train Loss: 0.012957 // Val Loss: 0.085086 // Train Acc: 99.90% // Val Acc: 97.58%\n",
      "Epoch 316: Train Loss: 0.012889 // Val Loss: 0.084898 // Train Acc: 99.90% // Val Acc: 97.56%\n",
      "Epoch 317: Train Loss: 0.012816 // Val Loss: 0.085091 // Train Acc: 99.89% // Val Acc: 97.55%\n",
      "Epoch 318: Train Loss: 0.012714 // Val Loss: 0.085094 // Train Acc: 99.90% // Val Acc: 97.58%\n",
      "Epoch 319: Train Loss: 0.012657 // Val Loss: 0.085242 // Train Acc: 99.90% // Val Acc: 97.52%\n",
      "Epoch 320: Train Loss: 0.012589 // Val Loss: 0.084744 // Train Acc: 99.90% // Val Acc: 97.58%\n",
      "Epoch 321: Train Loss: 0.012523 // Val Loss: 0.085146 // Train Acc: 99.90% // Val Acc: 97.54%\n",
      "Epoch 322: Train Loss: 0.012439 // Val Loss: 0.085200 // Train Acc: 99.91% // Val Acc: 97.60%\n",
      "Epoch 323: Train Loss: 0.012381 // Val Loss: 0.085230 // Train Acc: 99.90% // Val Acc: 97.58%\n",
      "Epoch 324: Train Loss: 0.012315 // Val Loss: 0.085278 // Train Acc: 99.91% // Val Acc: 97.54%\n",
      "Epoch 325: Train Loss: 0.012230 // Val Loss: 0.085677 // Train Acc: 99.91% // Val Acc: 97.54%\n",
      "Epoch 326: Train Loss: 0.012176 // Val Loss: 0.085725 // Train Acc: 99.91% // Val Acc: 97.53%\n",
      "Epoch 327: Train Loss: 0.012119 // Val Loss: 0.085249 // Train Acc: 99.91% // Val Acc: 97.59%\n",
      "Epoch 328: Train Loss: 0.012054 // Val Loss: 0.085630 // Train Acc: 99.91% // Val Acc: 97.54%\n",
      "Epoch 329: Train Loss: 0.011967 // Val Loss: 0.085508 // Train Acc: 99.92% // Val Acc: 97.56%\n",
      "Epoch 330: Train Loss: 0.011920 // Val Loss: 0.085403 // Train Acc: 99.91% // Val Acc: 97.54%\n",
      "Epoch 331: Train Loss: 0.011850 // Val Loss: 0.085602 // Train Acc: 99.92% // Val Acc: 97.58%\n",
      "Epoch 332: Train Loss: 0.011776 // Val Loss: 0.085423 // Train Acc: 99.91% // Val Acc: 97.52%\n",
      "Epoch 333: Train Loss: 0.011708 // Val Loss: 0.085716 // Train Acc: 99.92% // Val Acc: 97.56%\n",
      "Epoch 334: Train Loss: 0.011658 // Val Loss: 0.085559 // Train Acc: 99.92% // Val Acc: 97.58%\n",
      "Epoch 335: Train Loss: 0.011599 // Val Loss: 0.085733 // Train Acc: 99.91% // Val Acc: 97.58%\n",
      "Epoch 336: Train Loss: 0.011519 // Val Loss: 0.085562 // Train Acc: 99.91% // Val Acc: 97.60%\n",
      "Epoch 337: Train Loss: 0.011459 // Val Loss: 0.085580 // Train Acc: 99.92% // Val Acc: 97.59%\n",
      "Epoch 338: Train Loss: 0.011401 // Val Loss: 0.085645 // Train Acc: 99.92% // Val Acc: 97.57%\n",
      "Epoch 339: Train Loss: 0.011343 // Val Loss: 0.085939 // Train Acc: 99.92% // Val Acc: 97.52%\n",
      "Epoch 340: Train Loss: 0.011294 // Val Loss: 0.085541 // Train Acc: 99.93% // Val Acc: 97.56%\n",
      "Epoch 341: Train Loss: 0.011221 // Val Loss: 0.085896 // Train Acc: 99.92% // Val Acc: 97.58%\n",
      "Epoch 342: Train Loss: 0.011159 // Val Loss: 0.085646 // Train Acc: 99.92% // Val Acc: 97.57%\n",
      "Epoch 343: Train Loss: 0.011113 // Val Loss: 0.085963 // Train Acc: 99.92% // Val Acc: 97.55%\n",
      "Epoch 344: Train Loss: 0.011029 // Val Loss: 0.085930 // Train Acc: 99.93% // Val Acc: 97.58%\n",
      "Epoch 345: Train Loss: 0.010972 // Val Loss: 0.086089 // Train Acc: 99.93% // Val Acc: 97.52%\n",
      "Epoch 346: Train Loss: 0.010923 // Val Loss: 0.086279 // Train Acc: 99.93% // Val Acc: 97.55%\n",
      "Epoch 347: Train Loss: 0.010873 // Val Loss: 0.085953 // Train Acc: 99.93% // Val Acc: 97.56%\n",
      "Epoch 348: Train Loss: 0.010781 // Val Loss: 0.085868 // Train Acc: 99.93% // Val Acc: 97.56%\n",
      "Epoch 349: Train Loss: 0.010763 // Val Loss: 0.085948 // Train Acc: 99.94% // Val Acc: 97.56%\n",
      "Epoch 350: Train Loss: 0.010688 // Val Loss: 0.086736 // Train Acc: 99.93% // Val Acc: 97.53%\n",
      "Epoch 351: Train Loss: 0.010632 // Val Loss: 0.086204 // Train Acc: 99.94% // Val Acc: 97.59%\n",
      "Epoch 352: Train Loss: 0.010589 // Val Loss: 0.086343 // Train Acc: 99.93% // Val Acc: 97.57%\n",
      "Epoch 353: Train Loss: 0.010528 // Val Loss: 0.086339 // Train Acc: 99.93% // Val Acc: 97.59%\n",
      "Epoch 354: Train Loss: 0.010472 // Val Loss: 0.086177 // Train Acc: 99.93% // Val Acc: 97.58%\n",
      "Epoch 355: Train Loss: 0.010389 // Val Loss: 0.086313 // Train Acc: 99.94% // Val Acc: 97.57%\n",
      "Epoch 356: Train Loss: 0.010370 // Val Loss: 0.086497 // Train Acc: 99.94% // Val Acc: 97.58%\n",
      "Epoch 357: Train Loss: 0.010306 // Val Loss: 0.086108 // Train Acc: 99.93% // Val Acc: 97.61%\n",
      "Epoch 358: Train Loss: 0.010239 // Val Loss: 0.086077 // Train Acc: 99.94% // Val Acc: 97.57%\n",
      "Epoch 359: Train Loss: 0.010197 // Val Loss: 0.086394 // Train Acc: 99.94% // Val Acc: 97.56%\n",
      "Epoch 360: Train Loss: 0.010167 // Val Loss: 0.086606 // Train Acc: 99.94% // Val Acc: 97.59%\n",
      "Epoch 361: Train Loss: 0.010096 // Val Loss: 0.086575 // Train Acc: 99.94% // Val Acc: 97.58%\n",
      "Epoch 362: Train Loss: 0.010060 // Val Loss: 0.086635 // Train Acc: 99.95% // Val Acc: 97.56%\n",
      "Epoch 363: Train Loss: 0.009997 // Val Loss: 0.086227 // Train Acc: 99.95% // Val Acc: 97.60%\n",
      "Epoch 364: Train Loss: 0.009950 // Val Loss: 0.086540 // Train Acc: 99.94% // Val Acc: 97.62%\n",
      "Epoch 365: Train Loss: 0.009902 // Val Loss: 0.086591 // Train Acc: 99.95% // Val Acc: 97.61%\n",
      "Epoch 366: Train Loss: 0.009838 // Val Loss: 0.086336 // Train Acc: 99.95% // Val Acc: 97.60%\n",
      "Epoch 367: Train Loss: 0.009789 // Val Loss: 0.086722 // Train Acc: 99.95% // Val Acc: 97.58%\n",
      "Epoch 368: Train Loss: 0.009741 // Val Loss: 0.086430 // Train Acc: 99.95% // Val Acc: 97.58%\n",
      "Epoch 369: Train Loss: 0.009704 // Val Loss: 0.086566 // Train Acc: 99.95% // Val Acc: 97.61%\n",
      "Epoch 370: Train Loss: 0.009662 // Val Loss: 0.086768 // Train Acc: 99.96% // Val Acc: 97.58%\n",
      "Epoch 371: Train Loss: 0.009597 // Val Loss: 0.086416 // Train Acc: 99.95% // Val Acc: 97.58%\n",
      "Epoch 372: Train Loss: 0.009552 // Val Loss: 0.086316 // Train Acc: 99.96% // Val Acc: 97.57%\n",
      "Epoch 373: Train Loss: 0.009507 // Val Loss: 0.086447 // Train Acc: 99.96% // Val Acc: 97.58%\n",
      "Epoch 374: Train Loss: 0.009455 // Val Loss: 0.086779 // Train Acc: 99.96% // Val Acc: 97.61%\n",
      "Epoch 375: Train Loss: 0.009410 // Val Loss: 0.086567 // Train Acc: 99.95% // Val Acc: 97.62%\n",
      "Epoch 376: Train Loss: 0.009359 // Val Loss: 0.086638 // Train Acc: 99.95% // Val Acc: 97.62%\n",
      "Epoch 377: Train Loss: 0.009310 // Val Loss: 0.086970 // Train Acc: 99.95% // Val Acc: 97.62%\n",
      "Epoch 378: Train Loss: 0.009265 // Val Loss: 0.086806 // Train Acc: 99.96% // Val Acc: 97.60%\n",
      "Epoch 379: Train Loss: 0.009229 // Val Loss: 0.086882 // Train Acc: 99.96% // Val Acc: 97.56%\n",
      "Epoch 380: Train Loss: 0.009178 // Val Loss: 0.086881 // Train Acc: 99.96% // Val Acc: 97.61%\n",
      "Epoch 381: Train Loss: 0.009142 // Val Loss: 0.086685 // Train Acc: 99.96% // Val Acc: 97.60%\n",
      "Epoch 382: Train Loss: 0.009090 // Val Loss: 0.087205 // Train Acc: 99.96% // Val Acc: 97.62%\n",
      "Epoch 383: Train Loss: 0.009035 // Val Loss: 0.086955 // Train Acc: 99.96% // Val Acc: 97.62%\n",
      "Epoch 384: Train Loss: 0.009010 // Val Loss: 0.087210 // Train Acc: 99.96% // Val Acc: 97.61%\n",
      "Epoch 385: Train Loss: 0.008962 // Val Loss: 0.087034 // Train Acc: 99.96% // Val Acc: 97.59%\n",
      "Epoch 386: Train Loss: 0.008910 // Val Loss: 0.087062 // Train Acc: 99.96% // Val Acc: 97.58%\n",
      "Epoch 387: Train Loss: 0.008863 // Val Loss: 0.086897 // Train Acc: 99.96% // Val Acc: 97.65%\n",
      "Epoch 388: Train Loss: 0.008828 // Val Loss: 0.086925 // Train Acc: 99.96% // Val Acc: 97.59%\n",
      "Epoch 389: Train Loss: 0.008788 // Val Loss: 0.086925 // Train Acc: 99.96% // Val Acc: 97.62%\n",
      "Epoch 390: Train Loss: 0.008738 // Val Loss: 0.087201 // Train Acc: 99.96% // Val Acc: 97.58%\n",
      "Epoch 391: Train Loss: 0.008699 // Val Loss: 0.087286 // Train Acc: 99.97% // Val Acc: 97.63%\n",
      "Epoch 392: Train Loss: 0.008646 // Val Loss: 0.087190 // Train Acc: 99.96% // Val Acc: 97.61%\n",
      "Epoch 393: Train Loss: 0.008587 // Val Loss: 0.087413 // Train Acc: 99.97% // Val Acc: 97.59%\n",
      "Epoch 394: Train Loss: 0.008575 // Val Loss: 0.087321 // Train Acc: 99.96% // Val Acc: 97.58%\n",
      "Epoch 395: Train Loss: 0.008531 // Val Loss: 0.087231 // Train Acc: 99.96% // Val Acc: 97.61%\n",
      "Epoch 396: Train Loss: 0.008489 // Val Loss: 0.087404 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 397: Train Loss: 0.008447 // Val Loss: 0.087366 // Train Acc: 99.96% // Val Acc: 97.61%\n",
      "Epoch 398: Train Loss: 0.008401 // Val Loss: 0.087467 // Train Acc: 99.97% // Val Acc: 97.59%\n",
      "Epoch 399: Train Loss: 0.008378 // Val Loss: 0.087410 // Train Acc: 99.97% // Val Acc: 97.60%\n",
      "Epoch 400: Train Loss: 0.008325 // Val Loss: 0.087558 // Train Acc: 99.97% // Val Acc: 97.60%\n",
      "Epoch 401: Train Loss: 0.008282 // Val Loss: 0.087317 // Train Acc: 99.97% // Val Acc: 97.60%\n",
      "Epoch 402: Train Loss: 0.008236 // Val Loss: 0.087487 // Train Acc: 99.97% // Val Acc: 97.60%\n",
      "Epoch 403: Train Loss: 0.008212 // Val Loss: 0.087375 // Train Acc: 99.96% // Val Acc: 97.58%\n",
      "Epoch 404: Train Loss: 0.008164 // Val Loss: 0.087428 // Train Acc: 99.97% // Val Acc: 97.60%\n",
      "Epoch 405: Train Loss: 0.008123 // Val Loss: 0.087619 // Train Acc: 99.97% // Val Acc: 97.62%\n",
      "Epoch 406: Train Loss: 0.008093 // Val Loss: 0.087579 // Train Acc: 99.97% // Val Acc: 97.61%\n",
      "Epoch 407: Train Loss: 0.008061 // Val Loss: 0.087571 // Train Acc: 99.97% // Val Acc: 97.60%\n",
      "Epoch 408: Train Loss: 0.008032 // Val Loss: 0.087698 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 409: Train Loss: 0.007991 // Val Loss: 0.087782 // Train Acc: 99.97% // Val Acc: 97.62%\n",
      "Epoch 410: Train Loss: 0.007966 // Val Loss: 0.087940 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 411: Train Loss: 0.007906 // Val Loss: 0.087695 // Train Acc: 99.97% // Val Acc: 97.62%\n",
      "Epoch 412: Train Loss: 0.007874 // Val Loss: 0.087754 // Train Acc: 99.97% // Val Acc: 97.59%\n",
      "Epoch 413: Train Loss: 0.007844 // Val Loss: 0.087898 // Train Acc: 99.97% // Val Acc: 97.61%\n",
      "Epoch 414: Train Loss: 0.007799 // Val Loss: 0.087973 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 415: Train Loss: 0.007755 // Val Loss: 0.087990 // Train Acc: 99.97% // Val Acc: 97.56%\n",
      "Epoch 416: Train Loss: 0.007731 // Val Loss: 0.088208 // Train Acc: 99.97% // Val Acc: 97.61%\n",
      "Epoch 417: Train Loss: 0.007693 // Val Loss: 0.087815 // Train Acc: 99.97% // Val Acc: 97.62%\n",
      "Epoch 418: Train Loss: 0.007670 // Val Loss: 0.087802 // Train Acc: 99.97% // Val Acc: 97.61%\n",
      "Epoch 419: Train Loss: 0.007634 // Val Loss: 0.088125 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 420: Train Loss: 0.007601 // Val Loss: 0.088019 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 421: Train Loss: 0.007567 // Val Loss: 0.087936 // Train Acc: 99.97% // Val Acc: 97.62%\n",
      "Epoch 422: Train Loss: 0.007525 // Val Loss: 0.088225 // Train Acc: 99.97% // Val Acc: 97.60%\n",
      "Epoch 423: Train Loss: 0.007498 // Val Loss: 0.088117 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 424: Train Loss: 0.007458 // Val Loss: 0.088539 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 425: Train Loss: 0.007429 // Val Loss: 0.088049 // Train Acc: 99.97% // Val Acc: 97.62%\n",
      "Epoch 426: Train Loss: 0.007397 // Val Loss: 0.088291 // Train Acc: 99.97% // Val Acc: 97.61%\n",
      "Epoch 427: Train Loss: 0.007359 // Val Loss: 0.088141 // Train Acc: 99.98% // Val Acc: 97.60%\n",
      "Epoch 428: Train Loss: 0.007335 // Val Loss: 0.088231 // Train Acc: 99.97% // Val Acc: 97.61%\n",
      "Epoch 429: Train Loss: 0.007295 // Val Loss: 0.088314 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 430: Train Loss: 0.007269 // Val Loss: 0.088456 // Train Acc: 99.98% // Val Acc: 97.58%\n",
      "Epoch 431: Train Loss: 0.007227 // Val Loss: 0.088161 // Train Acc: 99.98% // Val Acc: 97.62%\n",
      "Epoch 432: Train Loss: 0.007193 // Val Loss: 0.088768 // Train Acc: 99.97% // Val Acc: 97.58%\n",
      "Epoch 433: Train Loss: 0.007176 // Val Loss: 0.088629 // Train Acc: 99.98% // Val Acc: 97.58%\n",
      "Epoch 434: Train Loss: 0.007138 // Val Loss: 0.088370 // Train Acc: 99.98% // Val Acc: 97.59%\n",
      "Epoch 435: Train Loss: 0.007096 // Val Loss: 0.088713 // Train Acc: 99.98% // Val Acc: 97.58%\n",
      "Epoch 436: Train Loss: 0.007073 // Val Loss: 0.088576 // Train Acc: 99.98% // Val Acc: 97.58%\n",
      "Epoch 437: Train Loss: 0.007037 // Val Loss: 0.088536 // Train Acc: 99.98% // Val Acc: 97.58%\n",
      "Epoch 438: Train Loss: 0.007004 // Val Loss: 0.088753 // Train Acc: 99.98% // Val Acc: 97.60%\n",
      "Epoch 439: Train Loss: 0.006986 // Val Loss: 0.088662 // Train Acc: 99.98% // Val Acc: 97.59%\n",
      "Epoch 440: Train Loss: 0.006954 // Val Loss: 0.088672 // Train Acc: 99.98% // Val Acc: 97.56%\n",
      "Epoch 441: Train Loss: 0.006925 // Val Loss: 0.088777 // Train Acc: 99.98% // Val Acc: 97.58%\n",
      "Epoch 442: Train Loss: 0.006892 // Val Loss: 0.088774 // Train Acc: 99.98% // Val Acc: 97.62%\n",
      "Epoch 443: Train Loss: 0.006873 // Val Loss: 0.088656 // Train Acc: 99.98% // Val Acc: 97.61%\n",
      "Epoch 444: Train Loss: 0.006839 // Val Loss: 0.088854 // Train Acc: 99.98% // Val Acc: 97.58%\n",
      "Epoch 445: Train Loss: 0.006810 // Val Loss: 0.088800 // Train Acc: 99.98% // Val Acc: 97.60%\n",
      "Epoch 446: Train Loss: 0.006769 // Val Loss: 0.088999 // Train Acc: 99.98% // Val Acc: 97.58%\n",
      "Epoch 447: Train Loss: 0.006743 // Val Loss: 0.089063 // Train Acc: 99.98% // Val Acc: 97.55%\n",
      "Epoch 448: Train Loss: 0.006724 // Val Loss: 0.088920 // Train Acc: 99.98% // Val Acc: 97.58%\n",
      "Epoch 449: Train Loss: 0.006703 // Val Loss: 0.088970 // Train Acc: 99.98% // Val Acc: 97.57%\n",
      "Epoch 450: Train Loss: 0.006665 // Val Loss: 0.089092 // Train Acc: 99.99% // Val Acc: 97.59%\n",
      "Epoch 451: Train Loss: 0.006633 // Val Loss: 0.088866 // Train Acc: 99.99% // Val Acc: 97.60%\n",
      "Epoch 452: Train Loss: 0.006603 // Val Loss: 0.089280 // Train Acc: 99.99% // Val Acc: 97.59%\n",
      "Epoch 453: Train Loss: 0.006580 // Val Loss: 0.089262 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 454: Train Loss: 0.006563 // Val Loss: 0.089172 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 455: Train Loss: 0.006533 // Val Loss: 0.089245 // Train Acc: 99.99% // Val Acc: 97.57%\n",
      "Epoch 456: Train Loss: 0.006511 // Val Loss: 0.089315 // Train Acc: 99.99% // Val Acc: 97.60%\n",
      "Epoch 457: Train Loss: 0.006486 // Val Loss: 0.089043 // Train Acc: 99.99% // Val Acc: 97.59%\n",
      "Epoch 458: Train Loss: 0.006444 // Val Loss: 0.089230 // Train Acc: 99.99% // Val Acc: 97.61%\n",
      "Epoch 459: Train Loss: 0.006421 // Val Loss: 0.089185 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 460: Train Loss: 0.006392 // Val Loss: 0.089358 // Train Acc: 99.99% // Val Acc: 97.61%\n",
      "Epoch 461: Train Loss: 0.006373 // Val Loss: 0.089528 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 462: Train Loss: 0.006352 // Val Loss: 0.089389 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 463: Train Loss: 0.006322 // Val Loss: 0.089433 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 464: Train Loss: 0.006299 // Val Loss: 0.089403 // Train Acc: 99.98% // Val Acc: 97.61%\n",
      "Epoch 465: Train Loss: 0.006275 // Val Loss: 0.089311 // Train Acc: 99.99% // Val Acc: 97.59%\n",
      "Epoch 466: Train Loss: 0.006231 // Val Loss: 0.089658 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 467: Train Loss: 0.006215 // Val Loss: 0.089297 // Train Acc: 99.99% // Val Acc: 97.61%\n",
      "Epoch 468: Train Loss: 0.006208 // Val Loss: 0.089520 // Train Acc: 99.98% // Val Acc: 97.57%\n",
      "Epoch 469: Train Loss: 0.006170 // Val Loss: 0.089740 // Train Acc: 99.99% // Val Acc: 97.60%\n",
      "Epoch 470: Train Loss: 0.006145 // Val Loss: 0.089606 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 471: Train Loss: 0.006119 // Val Loss: 0.089667 // Train Acc: 99.99% // Val Acc: 97.57%\n",
      "Epoch 472: Train Loss: 0.006104 // Val Loss: 0.089572 // Train Acc: 99.99% // Val Acc: 97.57%\n",
      "Epoch 473: Train Loss: 0.006078 // Val Loss: 0.089755 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 474: Train Loss: 0.006053 // Val Loss: 0.090042 // Train Acc: 99.99% // Val Acc: 97.57%\n",
      "Epoch 475: Train Loss: 0.006031 // Val Loss: 0.089519 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 476: Train Loss: 0.006012 // Val Loss: 0.089803 // Train Acc: 99.99% // Val Acc: 97.57%\n",
      "Epoch 477: Train Loss: 0.005984 // Val Loss: 0.089975 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 478: Train Loss: 0.005962 // Val Loss: 0.089913 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 479: Train Loss: 0.005932 // Val Loss: 0.089704 // Train Acc: 99.99% // Val Acc: 97.56%\n",
      "Epoch 480: Train Loss: 0.005914 // Val Loss: 0.090025 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 481: Train Loss: 0.005889 // Val Loss: 0.090212 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 482: Train Loss: 0.005870 // Val Loss: 0.089946 // Train Acc: 99.99% // Val Acc: 97.59%\n",
      "Epoch 483: Train Loss: 0.005846 // Val Loss: 0.089869 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 484: Train Loss: 0.005825 // Val Loss: 0.089982 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 485: Train Loss: 0.005809 // Val Loss: 0.090302 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 486: Train Loss: 0.005783 // Val Loss: 0.090023 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 487: Train Loss: 0.005752 // Val Loss: 0.090003 // Train Acc: 100.00% // Val Acc: 97.61%\n",
      "Epoch 488: Train Loss: 0.005731 // Val Loss: 0.090173 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 489: Train Loss: 0.005714 // Val Loss: 0.090170 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 490: Train Loss: 0.005690 // Val Loss: 0.090305 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 491: Train Loss: 0.005678 // Val Loss: 0.090266 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 492: Train Loss: 0.005653 // Val Loss: 0.090140 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 493: Train Loss: 0.005621 // Val Loss: 0.090267 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 494: Train Loss: 0.005604 // Val Loss: 0.090225 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 495: Train Loss: 0.005595 // Val Loss: 0.090195 // Train Acc: 99.99% // Val Acc: 97.58%\n",
      "Epoch 496: Train Loss: 0.005566 // Val Loss: 0.090611 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 497: Train Loss: 0.005550 // Val Loss: 0.090324 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 498: Train Loss: 0.005532 // Val Loss: 0.090393 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 499: Train Loss: 0.005506 // Val Loss: 0.090617 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 500: Train Loss: 0.005493 // Val Loss: 0.090364 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 501: Train Loss: 0.005467 // Val Loss: 0.090577 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 502: Train Loss: 0.005445 // Val Loss: 0.090804 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 503: Train Loss: 0.005431 // Val Loss: 0.090382 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 504: Train Loss: 0.005413 // Val Loss: 0.090508 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 505: Train Loss: 0.005390 // Val Loss: 0.090620 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 506: Train Loss: 0.005365 // Val Loss: 0.090582 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 507: Train Loss: 0.005348 // Val Loss: 0.090580 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 508: Train Loss: 0.005322 // Val Loss: 0.090848 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 509: Train Loss: 0.005313 // Val Loss: 0.090728 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 510: Train Loss: 0.005290 // Val Loss: 0.090696 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 511: Train Loss: 0.005274 // Val Loss: 0.090712 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 512: Train Loss: 0.005254 // Val Loss: 0.090807 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 513: Train Loss: 0.005237 // Val Loss: 0.090849 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 514: Train Loss: 0.005219 // Val Loss: 0.090838 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 515: Train Loss: 0.005206 // Val Loss: 0.090900 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 516: Train Loss: 0.005175 // Val Loss: 0.091145 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 517: Train Loss: 0.005165 // Val Loss: 0.091163 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 518: Train Loss: 0.005151 // Val Loss: 0.090829 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 519: Train Loss: 0.005134 // Val Loss: 0.090991 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 520: Train Loss: 0.005106 // Val Loss: 0.091070 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 521: Train Loss: 0.005097 // Val Loss: 0.091240 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 522: Train Loss: 0.005075 // Val Loss: 0.091445 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 523: Train Loss: 0.005062 // Val Loss: 0.091137 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 524: Train Loss: 0.005045 // Val Loss: 0.091100 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 525: Train Loss: 0.005018 // Val Loss: 0.091250 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 526: Train Loss: 0.005012 // Val Loss: 0.091379 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 527: Train Loss: 0.004990 // Val Loss: 0.091369 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 528: Train Loss: 0.004979 // Val Loss: 0.091305 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 529: Train Loss: 0.004961 // Val Loss: 0.091402 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 530: Train Loss: 0.004934 // Val Loss: 0.091324 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 531: Train Loss: 0.004924 // Val Loss: 0.091267 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 532: Train Loss: 0.004901 // Val Loss: 0.091817 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 533: Train Loss: 0.004891 // Val Loss: 0.091461 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 534: Train Loss: 0.004875 // Val Loss: 0.091622 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 535: Train Loss: 0.004861 // Val Loss: 0.091313 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 536: Train Loss: 0.004844 // Val Loss: 0.091443 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 537: Train Loss: 0.004824 // Val Loss: 0.091374 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 538: Train Loss: 0.004809 // Val Loss: 0.091377 // Train Acc: 100.00% // Val Acc: 97.61%\n",
      "Epoch 539: Train Loss: 0.004796 // Val Loss: 0.091585 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 540: Train Loss: 0.004780 // Val Loss: 0.091468 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 541: Train Loss: 0.004757 // Val Loss: 0.091537 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 542: Train Loss: 0.004745 // Val Loss: 0.091786 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 543: Train Loss: 0.004721 // Val Loss: 0.091608 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 544: Train Loss: 0.004719 // Val Loss: 0.091630 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 545: Train Loss: 0.004700 // Val Loss: 0.091890 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 546: Train Loss: 0.004683 // Val Loss: 0.091641 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 547: Train Loss: 0.004672 // Val Loss: 0.091803 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 548: Train Loss: 0.004658 // Val Loss: 0.091851 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 549: Train Loss: 0.004636 // Val Loss: 0.091891 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 550: Train Loss: 0.004625 // Val Loss: 0.091870 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 551: Train Loss: 0.004604 // Val Loss: 0.091951 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 552: Train Loss: 0.004597 // Val Loss: 0.091957 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 553: Train Loss: 0.004576 // Val Loss: 0.091991 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 554: Train Loss: 0.004568 // Val Loss: 0.091761 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 555: Train Loss: 0.004556 // Val Loss: 0.092121 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 556: Train Loss: 0.004536 // Val Loss: 0.092038 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 557: Train Loss: 0.004523 // Val Loss: 0.092164 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 558: Train Loss: 0.004509 // Val Loss: 0.092206 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 559: Train Loss: 0.004496 // Val Loss: 0.092106 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 560: Train Loss: 0.004478 // Val Loss: 0.092122 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 561: Train Loss: 0.004468 // Val Loss: 0.092184 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 562: Train Loss: 0.004449 // Val Loss: 0.092240 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 563: Train Loss: 0.004439 // Val Loss: 0.092221 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 564: Train Loss: 0.004420 // Val Loss: 0.092344 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 565: Train Loss: 0.004415 // Val Loss: 0.092128 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 566: Train Loss: 0.004397 // Val Loss: 0.092336 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 567: Train Loss: 0.004385 // Val Loss: 0.092405 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 568: Train Loss: 0.004373 // Val Loss: 0.092329 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 569: Train Loss: 0.004359 // Val Loss: 0.092217 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 570: Train Loss: 0.004343 // Val Loss: 0.092308 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 571: Train Loss: 0.004331 // Val Loss: 0.092405 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 572: Train Loss: 0.004317 // Val Loss: 0.092614 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 573: Train Loss: 0.004301 // Val Loss: 0.092605 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 574: Train Loss: 0.004293 // Val Loss: 0.092521 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 575: Train Loss: 0.004275 // Val Loss: 0.092384 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 576: Train Loss: 0.004259 // Val Loss: 0.092525 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 577: Train Loss: 0.004254 // Val Loss: 0.092607 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 578: Train Loss: 0.004242 // Val Loss: 0.092484 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 579: Train Loss: 0.004225 // Val Loss: 0.092509 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 580: Train Loss: 0.004215 // Val Loss: 0.092833 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 581: Train Loss: 0.004202 // Val Loss: 0.092733 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 582: Train Loss: 0.004187 // Val Loss: 0.092629 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 583: Train Loss: 0.004174 // Val Loss: 0.092758 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 584: Train Loss: 0.004163 // Val Loss: 0.092827 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 585: Train Loss: 0.004155 // Val Loss: 0.092825 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 586: Train Loss: 0.004139 // Val Loss: 0.092696 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 587: Train Loss: 0.004131 // Val Loss: 0.092974 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 588: Train Loss: 0.004116 // Val Loss: 0.092877 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 589: Train Loss: 0.004100 // Val Loss: 0.092815 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 590: Train Loss: 0.004089 // Val Loss: 0.092978 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 591: Train Loss: 0.004078 // Val Loss: 0.092986 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 592: Train Loss: 0.004068 // Val Loss: 0.092994 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 593: Train Loss: 0.004055 // Val Loss: 0.093147 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 594: Train Loss: 0.004038 // Val Loss: 0.093165 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 595: Train Loss: 0.004027 // Val Loss: 0.092939 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 596: Train Loss: 0.004019 // Val Loss: 0.093030 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 597: Train Loss: 0.004006 // Val Loss: 0.093084 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 598: Train Loss: 0.003995 // Val Loss: 0.093051 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 599: Train Loss: 0.003983 // Val Loss: 0.093187 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 600: Train Loss: 0.003972 // Val Loss: 0.093363 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 601: Train Loss: 0.003962 // Val Loss: 0.093242 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 602: Train Loss: 0.003947 // Val Loss: 0.093239 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 603: Train Loss: 0.003940 // Val Loss: 0.093145 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 604: Train Loss: 0.003922 // Val Loss: 0.093267 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 605: Train Loss: 0.003915 // Val Loss: 0.093286 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 606: Train Loss: 0.003902 // Val Loss: 0.093396 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 607: Train Loss: 0.003894 // Val Loss: 0.093437 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 608: Train Loss: 0.003883 // Val Loss: 0.093543 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 609: Train Loss: 0.003870 // Val Loss: 0.093232 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 610: Train Loss: 0.003863 // Val Loss: 0.093651 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 611: Train Loss: 0.003850 // Val Loss: 0.093303 // Train Acc: 100.00% // Val Acc: 97.62%\n",
      "Epoch 612: Train Loss: 0.003838 // Val Loss: 0.093414 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 613: Train Loss: 0.003832 // Val Loss: 0.093481 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 614: Train Loss: 0.003813 // Val Loss: 0.093675 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 615: Train Loss: 0.003813 // Val Loss: 0.093567 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 616: Train Loss: 0.003797 // Val Loss: 0.093488 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 617: Train Loss: 0.003787 // Val Loss: 0.093595 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 618: Train Loss: 0.003773 // Val Loss: 0.093641 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 619: Train Loss: 0.003767 // Val Loss: 0.093654 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 620: Train Loss: 0.003753 // Val Loss: 0.093660 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 621: Train Loss: 0.003742 // Val Loss: 0.093511 // Train Acc: 100.00% // Val Acc: 97.61%\n",
      "Epoch 622: Train Loss: 0.003733 // Val Loss: 0.093772 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 623: Train Loss: 0.003722 // Val Loss: 0.093692 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 624: Train Loss: 0.003707 // Val Loss: 0.093588 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 625: Train Loss: 0.003705 // Val Loss: 0.093778 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 626: Train Loss: 0.003692 // Val Loss: 0.093832 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 627: Train Loss: 0.003684 // Val Loss: 0.093896 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 628: Train Loss: 0.003674 // Val Loss: 0.093828 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 629: Train Loss: 0.003663 // Val Loss: 0.093811 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 630: Train Loss: 0.003653 // Val Loss: 0.093803 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 631: Train Loss: 0.003636 // Val Loss: 0.093910 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 632: Train Loss: 0.003635 // Val Loss: 0.093896 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 633: Train Loss: 0.003625 // Val Loss: 0.094029 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 634: Train Loss: 0.003615 // Val Loss: 0.093949 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 635: Train Loss: 0.003604 // Val Loss: 0.093887 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 636: Train Loss: 0.003597 // Val Loss: 0.094214 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 637: Train Loss: 0.003586 // Val Loss: 0.094182 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 638: Train Loss: 0.003577 // Val Loss: 0.094051 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 639: Train Loss: 0.003565 // Val Loss: 0.093996 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 640: Train Loss: 0.003555 // Val Loss: 0.094099 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 641: Train Loss: 0.003549 // Val Loss: 0.094149 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 642: Train Loss: 0.003536 // Val Loss: 0.094199 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 643: Train Loss: 0.003522 // Val Loss: 0.094171 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 644: Train Loss: 0.003520 // Val Loss: 0.094191 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 645: Train Loss: 0.003508 // Val Loss: 0.094262 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 646: Train Loss: 0.003499 // Val Loss: 0.094175 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 647: Train Loss: 0.003492 // Val Loss: 0.094210 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 648: Train Loss: 0.003483 // Val Loss: 0.094328 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 649: Train Loss: 0.003471 // Val Loss: 0.094294 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 650: Train Loss: 0.003460 // Val Loss: 0.094382 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 651: Train Loss: 0.003457 // Val Loss: 0.094447 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 652: Train Loss: 0.003447 // Val Loss: 0.094493 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 653: Train Loss: 0.003438 // Val Loss: 0.094403 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 654: Train Loss: 0.003430 // Val Loss: 0.094419 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 655: Train Loss: 0.003418 // Val Loss: 0.094522 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 656: Train Loss: 0.003410 // Val Loss: 0.094401 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 657: Train Loss: 0.003404 // Val Loss: 0.094490 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 658: Train Loss: 0.003392 // Val Loss: 0.094579 // Train Acc: 100.00% // Val Acc: 97.54%\n",
      "Epoch 659: Train Loss: 0.003385 // Val Loss: 0.094489 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 660: Train Loss: 0.003377 // Val Loss: 0.094653 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 661: Train Loss: 0.003367 // Val Loss: 0.094612 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 662: Train Loss: 0.003356 // Val Loss: 0.094873 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 663: Train Loss: 0.003353 // Val Loss: 0.094711 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 664: Train Loss: 0.003342 // Val Loss: 0.094596 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 665: Train Loss: 0.003331 // Val Loss: 0.094667 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 666: Train Loss: 0.003327 // Val Loss: 0.094743 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 667: Train Loss: 0.003317 // Val Loss: 0.094903 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 668: Train Loss: 0.003307 // Val Loss: 0.094822 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 669: Train Loss: 0.003302 // Val Loss: 0.094855 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 670: Train Loss: 0.003292 // Val Loss: 0.094762 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 671: Train Loss: 0.003286 // Val Loss: 0.094885 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 672: Train Loss: 0.003274 // Val Loss: 0.094928 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 673: Train Loss: 0.003267 // Val Loss: 0.094909 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 674: Train Loss: 0.003259 // Val Loss: 0.095028 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 675: Train Loss: 0.003251 // Val Loss: 0.094912 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 676: Train Loss: 0.003245 // Val Loss: 0.095043 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 677: Train Loss: 0.003231 // Val Loss: 0.095064 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 678: Train Loss: 0.003226 // Val Loss: 0.095154 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 679: Train Loss: 0.003217 // Val Loss: 0.095109 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 680: Train Loss: 0.003213 // Val Loss: 0.095106 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 681: Train Loss: 0.003202 // Val Loss: 0.095146 // Train Acc: 100.00% // Val Acc: 97.62%\n",
      "Epoch 682: Train Loss: 0.003195 // Val Loss: 0.095111 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 683: Train Loss: 0.003187 // Val Loss: 0.095183 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 684: Train Loss: 0.003179 // Val Loss: 0.095086 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 685: Train Loss: 0.003172 // Val Loss: 0.095347 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 686: Train Loss: 0.003165 // Val Loss: 0.095354 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 687: Train Loss: 0.003155 // Val Loss: 0.095306 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 688: Train Loss: 0.003149 // Val Loss: 0.095314 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 689: Train Loss: 0.003143 // Val Loss: 0.095283 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 690: Train Loss: 0.003135 // Val Loss: 0.095335 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 691: Train Loss: 0.003122 // Val Loss: 0.095395 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 692: Train Loss: 0.003116 // Val Loss: 0.095391 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 693: Train Loss: 0.003112 // Val Loss: 0.095423 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 694: Train Loss: 0.003101 // Val Loss: 0.095293 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 695: Train Loss: 0.003092 // Val Loss: 0.095537 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 696: Train Loss: 0.003088 // Val Loss: 0.095613 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 697: Train Loss: 0.003080 // Val Loss: 0.095384 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 698: Train Loss: 0.003075 // Val Loss: 0.095473 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 699: Train Loss: 0.003069 // Val Loss: 0.095555 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 700: Train Loss: 0.003061 // Val Loss: 0.095463 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 701: Train Loss: 0.003052 // Val Loss: 0.095606 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 702: Train Loss: 0.003045 // Val Loss: 0.095581 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 703: Train Loss: 0.003038 // Val Loss: 0.095560 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 704: Train Loss: 0.003030 // Val Loss: 0.095633 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 705: Train Loss: 0.003022 // Val Loss: 0.095614 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 706: Train Loss: 0.003014 // Val Loss: 0.095624 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 707: Train Loss: 0.003012 // Val Loss: 0.095635 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 708: Train Loss: 0.003001 // Val Loss: 0.095891 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 709: Train Loss: 0.002995 // Val Loss: 0.095810 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 710: Train Loss: 0.002987 // Val Loss: 0.095817 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 711: Train Loss: 0.002982 // Val Loss: 0.095772 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 712: Train Loss: 0.002971 // Val Loss: 0.095898 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 713: Train Loss: 0.002967 // Val Loss: 0.095805 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 714: Train Loss: 0.002958 // Val Loss: 0.095882 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 715: Train Loss: 0.002954 // Val Loss: 0.095859 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 716: Train Loss: 0.002948 // Val Loss: 0.095960 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 717: Train Loss: 0.002941 // Val Loss: 0.095945 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 718: Train Loss: 0.002934 // Val Loss: 0.096031 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 719: Train Loss: 0.002925 // Val Loss: 0.096029 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 720: Train Loss: 0.002920 // Val Loss: 0.096057 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 721: Train Loss: 0.002914 // Val Loss: 0.096109 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 722: Train Loss: 0.002906 // Val Loss: 0.096015 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 723: Train Loss: 0.002899 // Val Loss: 0.096110 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 724: Train Loss: 0.002894 // Val Loss: 0.096236 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 725: Train Loss: 0.002887 // Val Loss: 0.096178 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 726: Train Loss: 0.002879 // Val Loss: 0.096038 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 727: Train Loss: 0.002875 // Val Loss: 0.096126 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 728: Train Loss: 0.002867 // Val Loss: 0.096029 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 729: Train Loss: 0.002861 // Val Loss: 0.096097 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 730: Train Loss: 0.002854 // Val Loss: 0.096185 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 731: Train Loss: 0.002849 // Val Loss: 0.096268 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 732: Train Loss: 0.002842 // Val Loss: 0.096340 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 733: Train Loss: 0.002836 // Val Loss: 0.096316 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 734: Train Loss: 0.002828 // Val Loss: 0.096308 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 735: Train Loss: 0.002821 // Val Loss: 0.096272 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 736: Train Loss: 0.002816 // Val Loss: 0.096249 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 737: Train Loss: 0.002808 // Val Loss: 0.096540 // Train Acc: 100.00% // Val Acc: 97.54%\n",
      "Epoch 738: Train Loss: 0.002804 // Val Loss: 0.096510 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 739: Train Loss: 0.002797 // Val Loss: 0.096567 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 740: Train Loss: 0.002792 // Val Loss: 0.096467 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 741: Train Loss: 0.002786 // Val Loss: 0.096500 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 742: Train Loss: 0.002779 // Val Loss: 0.096422 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 743: Train Loss: 0.002774 // Val Loss: 0.096530 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 744: Train Loss: 0.002765 // Val Loss: 0.096609 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 745: Train Loss: 0.002761 // Val Loss: 0.096546 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 746: Train Loss: 0.002754 // Val Loss: 0.096639 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 747: Train Loss: 0.002749 // Val Loss: 0.096799 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 748: Train Loss: 0.002742 // Val Loss: 0.096548 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 749: Train Loss: 0.002738 // Val Loss: 0.096653 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 750: Train Loss: 0.002731 // Val Loss: 0.096515 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 751: Train Loss: 0.002725 // Val Loss: 0.096741 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 752: Train Loss: 0.002717 // Val Loss: 0.096636 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 753: Train Loss: 0.002712 // Val Loss: 0.096659 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 754: Train Loss: 0.002707 // Val Loss: 0.096738 // Train Acc: 100.00% // Val Acc: 97.55%\n",
      "Epoch 755: Train Loss: 0.002698 // Val Loss: 0.096709 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 756: Train Loss: 0.002698 // Val Loss: 0.096840 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 757: Train Loss: 0.002690 // Val Loss: 0.096703 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 758: Train Loss: 0.002683 // Val Loss: 0.096780 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 759: Train Loss: 0.002675 // Val Loss: 0.096746 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 760: Train Loss: 0.002670 // Val Loss: 0.096835 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 761: Train Loss: 0.002663 // Val Loss: 0.096926 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 762: Train Loss: 0.002661 // Val Loss: 0.096920 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 763: Train Loss: 0.002656 // Val Loss: 0.096956 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 764: Train Loss: 0.002649 // Val Loss: 0.097101 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 765: Train Loss: 0.002643 // Val Loss: 0.096988 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 766: Train Loss: 0.002637 // Val Loss: 0.096972 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 767: Train Loss: 0.002632 // Val Loss: 0.097144 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 768: Train Loss: 0.002627 // Val Loss: 0.097004 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 769: Train Loss: 0.002622 // Val Loss: 0.097169 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 770: Train Loss: 0.002615 // Val Loss: 0.097098 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 771: Train Loss: 0.002612 // Val Loss: 0.097095 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 772: Train Loss: 0.002606 // Val Loss: 0.097171 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 773: Train Loss: 0.002600 // Val Loss: 0.097101 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 774: Train Loss: 0.002595 // Val Loss: 0.097265 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 775: Train Loss: 0.002588 // Val Loss: 0.097018 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 776: Train Loss: 0.002581 // Val Loss: 0.097354 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 777: Train Loss: 0.002579 // Val Loss: 0.097254 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 778: Train Loss: 0.002574 // Val Loss: 0.097344 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 779: Train Loss: 0.002566 // Val Loss: 0.097319 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 780: Train Loss: 0.002559 // Val Loss: 0.097308 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 781: Train Loss: 0.002557 // Val Loss: 0.097380 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 782: Train Loss: 0.002552 // Val Loss: 0.097335 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 783: Train Loss: 0.002545 // Val Loss: 0.097240 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 784: Train Loss: 0.002542 // Val Loss: 0.097313 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 785: Train Loss: 0.002535 // Val Loss: 0.097332 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 786: Train Loss: 0.002532 // Val Loss: 0.097366 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 787: Train Loss: 0.002524 // Val Loss: 0.097400 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 788: Train Loss: 0.002520 // Val Loss: 0.097466 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 789: Train Loss: 0.002515 // Val Loss: 0.097585 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 790: Train Loss: 0.002510 // Val Loss: 0.097563 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 791: Train Loss: 0.002505 // Val Loss: 0.097514 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 792: Train Loss: 0.002499 // Val Loss: 0.097610 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 793: Train Loss: 0.002494 // Val Loss: 0.097474 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 794: Train Loss: 0.002488 // Val Loss: 0.097667 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 795: Train Loss: 0.002486 // Val Loss: 0.097605 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 796: Train Loss: 0.002478 // Val Loss: 0.097596 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 797: Train Loss: 0.002472 // Val Loss: 0.097644 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 798: Train Loss: 0.002471 // Val Loss: 0.097675 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 799: Train Loss: 0.002466 // Val Loss: 0.097722 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 800: Train Loss: 0.002459 // Val Loss: 0.097691 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 801: Train Loss: 0.002455 // Val Loss: 0.097704 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 802: Train Loss: 0.002449 // Val Loss: 0.097673 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 803: Train Loss: 0.002445 // Val Loss: 0.097795 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 804: Train Loss: 0.002440 // Val Loss: 0.097806 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 805: Train Loss: 0.002434 // Val Loss: 0.097810 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 806: Train Loss: 0.002429 // Val Loss: 0.097795 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 807: Train Loss: 0.002422 // Val Loss: 0.097813 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 808: Train Loss: 0.002422 // Val Loss: 0.097744 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 809: Train Loss: 0.002415 // Val Loss: 0.097940 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 810: Train Loss: 0.002412 // Val Loss: 0.097819 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 811: Train Loss: 0.002405 // Val Loss: 0.097983 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 812: Train Loss: 0.002400 // Val Loss: 0.097919 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 813: Train Loss: 0.002397 // Val Loss: 0.097888 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 814: Train Loss: 0.002392 // Val Loss: 0.097908 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 815: Train Loss: 0.002385 // Val Loss: 0.097877 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 816: Train Loss: 0.002383 // Val Loss: 0.097926 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 817: Train Loss: 0.002378 // Val Loss: 0.098011 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 818: Train Loss: 0.002373 // Val Loss: 0.098017 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 819: Train Loss: 0.002368 // Val Loss: 0.098108 // Train Acc: 100.00% // Val Acc: 97.56%\n",
      "Epoch 820: Train Loss: 0.002364 // Val Loss: 0.098151 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 821: Train Loss: 0.002359 // Val Loss: 0.098089 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 822: Train Loss: 0.002355 // Val Loss: 0.098168 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 823: Train Loss: 0.002349 // Val Loss: 0.098171 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 824: Train Loss: 0.002344 // Val Loss: 0.098155 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 825: Train Loss: 0.002343 // Val Loss: 0.098117 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 826: Train Loss: 0.002336 // Val Loss: 0.098147 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 827: Train Loss: 0.002330 // Val Loss: 0.098176 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 828: Train Loss: 0.002329 // Val Loss: 0.098370 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 829: Train Loss: 0.002322 // Val Loss: 0.098392 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 830: Train Loss: 0.002319 // Val Loss: 0.098147 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 831: Train Loss: 0.002312 // Val Loss: 0.098140 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 832: Train Loss: 0.002309 // Val Loss: 0.098239 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 833: Train Loss: 0.002304 // Val Loss: 0.098217 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 834: Train Loss: 0.002301 // Val Loss: 0.098315 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 835: Train Loss: 0.002296 // Val Loss: 0.098280 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 836: Train Loss: 0.002293 // Val Loss: 0.098341 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 837: Train Loss: 0.002287 // Val Loss: 0.098361 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 838: Train Loss: 0.002284 // Val Loss: 0.098471 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 839: Train Loss: 0.002280 // Val Loss: 0.098482 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 840: Train Loss: 0.002275 // Val Loss: 0.098509 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 841: Train Loss: 0.002270 // Val Loss: 0.098390 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 842: Train Loss: 0.002267 // Val Loss: 0.098450 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 843: Train Loss: 0.002260 // Val Loss: 0.098520 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 844: Train Loss: 0.002258 // Val Loss: 0.098611 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 845: Train Loss: 0.002253 // Val Loss: 0.098477 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 846: Train Loss: 0.002247 // Val Loss: 0.098672 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 847: Train Loss: 0.002245 // Val Loss: 0.098511 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 848: Train Loss: 0.002240 // Val Loss: 0.098586 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 849: Train Loss: 0.002235 // Val Loss: 0.098652 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 850: Train Loss: 0.002234 // Val Loss: 0.098650 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 851: Train Loss: 0.002227 // Val Loss: 0.098614 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 852: Train Loss: 0.002224 // Val Loss: 0.098633 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 853: Train Loss: 0.002220 // Val Loss: 0.098729 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 854: Train Loss: 0.002214 // Val Loss: 0.098962 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 855: Train Loss: 0.002213 // Val Loss: 0.098756 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 856: Train Loss: 0.002208 // Val Loss: 0.098788 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 857: Train Loss: 0.002204 // Val Loss: 0.098662 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 858: Train Loss: 0.002200 // Val Loss: 0.098749 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 859: Train Loss: 0.002195 // Val Loss: 0.098832 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 860: Train Loss: 0.002192 // Val Loss: 0.098970 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 861: Train Loss: 0.002189 // Val Loss: 0.098826 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 862: Train Loss: 0.002185 // Val Loss: 0.098797 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 863: Train Loss: 0.002179 // Val Loss: 0.098840 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 864: Train Loss: 0.002175 // Val Loss: 0.098858 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 865: Train Loss: 0.002172 // Val Loss: 0.098913 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 866: Train Loss: 0.002169 // Val Loss: 0.099004 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 867: Train Loss: 0.002164 // Val Loss: 0.098953 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 868: Train Loss: 0.002160 // Val Loss: 0.098966 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 869: Train Loss: 0.002155 // Val Loss: 0.098956 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 870: Train Loss: 0.002151 // Val Loss: 0.098870 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 871: Train Loss: 0.002148 // Val Loss: 0.099072 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 872: Train Loss: 0.002144 // Val Loss: 0.099046 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 873: Train Loss: 0.002139 // Val Loss: 0.099041 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 874: Train Loss: 0.002137 // Val Loss: 0.099124 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 875: Train Loss: 0.002133 // Val Loss: 0.099136 // Train Acc: 100.00% // Val Acc: 97.57%\n",
      "Epoch 876: Train Loss: 0.002127 // Val Loss: 0.099108 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 877: Train Loss: 0.002124 // Val Loss: 0.099120 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 878: Train Loss: 0.002121 // Val Loss: 0.099061 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 879: Train Loss: 0.002117 // Val Loss: 0.099259 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 880: Train Loss: 0.002114 // Val Loss: 0.099248 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 881: Train Loss: 0.002109 // Val Loss: 0.099321 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 882: Train Loss: 0.002106 // Val Loss: 0.099310 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 883: Train Loss: 0.002102 // Val Loss: 0.099223 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 884: Train Loss: 0.002099 // Val Loss: 0.099200 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 885: Train Loss: 0.002094 // Val Loss: 0.099345 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 886: Train Loss: 0.002090 // Val Loss: 0.099231 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 887: Train Loss: 0.002088 // Val Loss: 0.099367 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 888: Train Loss: 0.002085 // Val Loss: 0.099401 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 889: Train Loss: 0.002080 // Val Loss: 0.099462 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 890: Train Loss: 0.002075 // Val Loss: 0.099366 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 891: Train Loss: 0.002073 // Val Loss: 0.099463 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 892: Train Loss: 0.002066 // Val Loss: 0.099378 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 893: Train Loss: 0.002065 // Val Loss: 0.099410 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 894: Train Loss: 0.002062 // Val Loss: 0.099483 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 895: Train Loss: 0.002057 // Val Loss: 0.099546 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 896: Train Loss: 0.002054 // Val Loss: 0.099516 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 897: Train Loss: 0.002050 // Val Loss: 0.099562 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 898: Train Loss: 0.002048 // Val Loss: 0.099566 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 899: Train Loss: 0.002043 // Val Loss: 0.099535 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 900: Train Loss: 0.002040 // Val Loss: 0.099567 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 901: Train Loss: 0.002036 // Val Loss: 0.099652 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 902: Train Loss: 0.002032 // Val Loss: 0.099676 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 903: Train Loss: 0.002029 // Val Loss: 0.099720 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 904: Train Loss: 0.002025 // Val Loss: 0.099699 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 905: Train Loss: 0.002022 // Val Loss: 0.099606 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 906: Train Loss: 0.002017 // Val Loss: 0.099705 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 907: Train Loss: 0.002015 // Val Loss: 0.099577 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 908: Train Loss: 0.002011 // Val Loss: 0.099591 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 909: Train Loss: 0.002009 // Val Loss: 0.099717 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 910: Train Loss: 0.002006 // Val Loss: 0.099713 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 911: Train Loss: 0.002001 // Val Loss: 0.099720 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 912: Train Loss: 0.001997 // Val Loss: 0.099752 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 913: Train Loss: 0.001994 // Val Loss: 0.099863 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 914: Train Loss: 0.001990 // Val Loss: 0.099780 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 915: Train Loss: 0.001988 // Val Loss: 0.099813 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 916: Train Loss: 0.001985 // Val Loss: 0.099889 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 917: Train Loss: 0.001982 // Val Loss: 0.099796 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 918: Train Loss: 0.001978 // Val Loss: 0.099855 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 919: Train Loss: 0.001974 // Val Loss: 0.099897 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 920: Train Loss: 0.001972 // Val Loss: 0.099831 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 921: Train Loss: 0.001968 // Val Loss: 0.100026 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 922: Train Loss: 0.001965 // Val Loss: 0.099893 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 923: Train Loss: 0.001960 // Val Loss: 0.100058 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 924: Train Loss: 0.001958 // Val Loss: 0.099997 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 925: Train Loss: 0.001954 // Val Loss: 0.100053 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 926: Train Loss: 0.001952 // Val Loss: 0.100091 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 927: Train Loss: 0.001947 // Val Loss: 0.099993 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 928: Train Loss: 0.001945 // Val Loss: 0.099990 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 929: Train Loss: 0.001942 // Val Loss: 0.100024 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 930: Train Loss: 0.001938 // Val Loss: 0.100115 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 931: Train Loss: 0.001936 // Val Loss: 0.100063 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 932: Train Loss: 0.001931 // Val Loss: 0.100134 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 933: Train Loss: 0.001929 // Val Loss: 0.100121 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 934: Train Loss: 0.001925 // Val Loss: 0.100206 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 935: Train Loss: 0.001922 // Val Loss: 0.100167 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 936: Train Loss: 0.001919 // Val Loss: 0.100242 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 937: Train Loss: 0.001915 // Val Loss: 0.100172 // Train Acc: 100.00% // Val Acc: 97.61%\n",
      "Epoch 938: Train Loss: 0.001912 // Val Loss: 0.100177 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 939: Train Loss: 0.001909 // Val Loss: 0.100237 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 940: Train Loss: 0.001906 // Val Loss: 0.100307 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 941: Train Loss: 0.001902 // Val Loss: 0.100214 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 942: Train Loss: 0.001899 // Val Loss: 0.100381 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 943: Train Loss: 0.001897 // Val Loss: 0.100285 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 944: Train Loss: 0.001894 // Val Loss: 0.100303 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 945: Train Loss: 0.001891 // Val Loss: 0.100345 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 946: Train Loss: 0.001888 // Val Loss: 0.100319 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 947: Train Loss: 0.001884 // Val Loss: 0.100443 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 948: Train Loss: 0.001881 // Val Loss: 0.100443 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 949: Train Loss: 0.001879 // Val Loss: 0.100394 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 950: Train Loss: 0.001875 // Val Loss: 0.100382 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 951: Train Loss: 0.001872 // Val Loss: 0.100437 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 952: Train Loss: 0.001869 // Val Loss: 0.100427 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 953: Train Loss: 0.001865 // Val Loss: 0.100604 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 954: Train Loss: 0.001864 // Val Loss: 0.100463 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 955: Train Loss: 0.001860 // Val Loss: 0.100564 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 956: Train Loss: 0.001858 // Val Loss: 0.100607 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 957: Train Loss: 0.001853 // Val Loss: 0.100578 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 958: Train Loss: 0.001852 // Val Loss: 0.100602 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 959: Train Loss: 0.001847 // Val Loss: 0.100658 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 960: Train Loss: 0.001846 // Val Loss: 0.100611 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 961: Train Loss: 0.001842 // Val Loss: 0.100531 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 962: Train Loss: 0.001839 // Val Loss: 0.100571 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 963: Train Loss: 0.001835 // Val Loss: 0.100653 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 964: Train Loss: 0.001832 // Val Loss: 0.100773 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 965: Train Loss: 0.001830 // Val Loss: 0.100718 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 966: Train Loss: 0.001828 // Val Loss: 0.100679 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 967: Train Loss: 0.001825 // Val Loss: 0.100626 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 968: Train Loss: 0.001822 // Val Loss: 0.100809 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 969: Train Loss: 0.001819 // Val Loss: 0.100829 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 970: Train Loss: 0.001815 // Val Loss: 0.100902 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 971: Train Loss: 0.001814 // Val Loss: 0.100748 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 972: Train Loss: 0.001810 // Val Loss: 0.100785 // Train Acc: 100.00% // Val Acc: 97.61%\n",
      "Epoch 973: Train Loss: 0.001807 // Val Loss: 0.100878 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 974: Train Loss: 0.001805 // Val Loss: 0.100871 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 975: Train Loss: 0.001801 // Val Loss: 0.100813 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 976: Train Loss: 0.001798 // Val Loss: 0.100932 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 977: Train Loss: 0.001796 // Val Loss: 0.100778 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 978: Train Loss: 0.001794 // Val Loss: 0.100879 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 979: Train Loss: 0.001790 // Val Loss: 0.100877 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 980: Train Loss: 0.001786 // Val Loss: 0.100969 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 981: Train Loss: 0.001784 // Val Loss: 0.100938 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 982: Train Loss: 0.001781 // Val Loss: 0.100889 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 983: Train Loss: 0.001779 // Val Loss: 0.100917 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 984: Train Loss: 0.001777 // Val Loss: 0.101065 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 985: Train Loss: 0.001773 // Val Loss: 0.101045 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 986: Train Loss: 0.001771 // Val Loss: 0.101003 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 987: Train Loss: 0.001768 // Val Loss: 0.100996 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 988: Train Loss: 0.001764 // Val Loss: 0.101066 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 989: Train Loss: 0.001763 // Val Loss: 0.101040 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 990: Train Loss: 0.001759 // Val Loss: 0.101057 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 991: Train Loss: 0.001757 // Val Loss: 0.101147 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 992: Train Loss: 0.001755 // Val Loss: 0.101054 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 993: Train Loss: 0.001752 // Val Loss: 0.101092 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 994: Train Loss: 0.001749 // Val Loss: 0.101110 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 995: Train Loss: 0.001746 // Val Loss: 0.101223 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 996: Train Loss: 0.001744 // Val Loss: 0.101145 // Train Acc: 100.00% // Val Acc: 97.60%\n",
      "Epoch 997: Train Loss: 0.001740 // Val Loss: 0.101159 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 998: Train Loss: 0.001738 // Val Loss: 0.101271 // Train Acc: 100.00% // Val Acc: 97.59%\n",
      "Epoch 999: Train Loss: 0.001734 // Val Loss: 0.101244 // Train Acc: 100.00% // Val Acc: 97.58%\n",
      "Epoch 1000: Train Loss: 0.001732 // Val Loss: 0.101368 // Train Acc: 100.00% // Val Acc: 97.58%\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000                 # number of times we iterate over the entire dataset\n",
    "total_val_losses = []       # list to store the validation losses at each epoch\n",
    "total_train_accuracies = []  # list to store training accuracies at each epoch\n",
    "total_val_accuracies = []    # list to store validation accuracies at each epoch\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    # total loss for each epoch         \n",
    "    total_loss = 0     \n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    # set the model to training mode. this is important because some layers \n",
    "    # (such as Dropout, BatchNorm) behave differently in training and testing mode.     \n",
    "    model.train()                           \n",
    "    \n",
    "    # iterate over the training set in batches of 64 images. each iteration\n",
    "    # returns a batch of 64 images and their labels.\n",
    "    for (batch_X, batch_y) in train_loader:\n",
    "        # zero out the gradients from the previous iteration. this is because\n",
    "        # pytorch accumulates gradients.\n",
    "        optimizer.zero_grad() \n",
    "        # forward pass. we get the outputs from the model using the inputs\n",
    "        # from the batch. these outputs are probabilities for each class.             \n",
    "        outputs = model(batch_X)\n",
    "        # compute the loss between the outputs and the labels in the batch.  \n",
    "        loss = criterion(outputs, batch_y)\n",
    "        # backward pass. we compute the gradients of the loss with respect to\n",
    "        # the learnable parameters of the model. \n",
    "        loss.backward()\n",
    "        # update the learnable parameters of the model using the gradients\n",
    "        # computed in the backward pass.\n",
    "        optimizer.step()\n",
    "        # add the loss of the batch to the total loss of the epoch.\n",
    "        total_loss += loss.item()\n",
    "        # calculate training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += batch_y.size(0)\n",
    "        correct_train += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # Validation/Testing Step.\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    # We do not want to update the learnable parameters of the model when\n",
    "    # we are evaluating the model. therefore, we set the model to evaluation\n",
    "    # mode. \n",
    "    model.eval()\n",
    "\n",
    "    # We do not want to compute the gradients when we are evaluating the model.\n",
    "    # therefore, we use torch.no_grad() to disable gradient computation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_X, batch_y) in test_loader:\n",
    "            # forward pass. we get the outputs from the model using the inputs\n",
    "            # from the batch. these outputs are probabilities for each class.\n",
    "            val_outputs = model(batch_X)  \n",
    "            # compute the loss between the outputs and the labels in the batch.\n",
    "            loss = criterion(val_outputs, batch_y)  \n",
    "            # add the loss of the batch to the total loss of the epoch.\n",
    "            val_loss += loss.item()\n",
    "            # calculate validation accuracy\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            total_val += batch_y.size(0)\n",
    "            correct_val += (predicted == batch_y).sum().item()\n",
    "            \n",
    "\n",
    "    \n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    \n",
    "    # compute the average metrics. we print these metrics for each epoch.\n",
    "    batch_train_loss = total_loss / len(train_loader)\n",
    "    batch_val_loss = val_loss / len(test_loader)\n",
    "    total_val_losses.append(batch_val_loss)\n",
    "    total_train_accuracies.append(train_accuracy)\n",
    "    total_val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # loss should go down!\n",
    "    print(f\"Epoch {epoch + 1:2d}: Train Loss: {batch_train_loss:.6f} // Val Loss: {batch_val_loss:.6f} // Train Acc: {train_accuracy:.2f}% // Val Acc: {val_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
